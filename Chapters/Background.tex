% Chapter Template

\chapter{Background} % Main chapter title

\label{Background} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


%-----------------------------------
%	SECTION 1
%-----------------------------------
\section{Introduction to Preposition Supersenses}
\subsubsection{*Adpositions signal semantic relations}
\subsubsection{*These relations are ambiguous}
\subsubsection{*Disambiguations could be beneficial to downstream tasks}
\subsection{The SNACS Scheme}
\subsubsection{SNACS Label Hierarchy}
\subsubsection{Construal Analysis}
\subsection{The STREUSLE corpus}
\subsection{Previous Approaches}
\subsection{PSS in This Work}
\pagebreak

%-----------------------------------
%	SECTION 2
%-----------------------------------
\section{Neural Networks for NLP}
\subsubsection{*Neural nets overview}
\subsubsection{*Activation functions}
\subsubsection{*Loss functions}
\subsubsection{*Training}
\subsubsection{*Hyperparameters vs. Model parameters, tuning}
\subsubsection{*Regularization and drop-out}
\subsection{Feed Forward Networks}
\subsection{Recurrent Neural Networks}
\subsubsection{Bidirectional RNN}
\subsubsection{Stacked RNNs}
\subsubsection{Long-Short-Term-Memory (LSTM)}
\subsection{Lookup Tables}
\subsection{The Stacked BiLSTM-MLP-Softmax Architecture}
\subsection{Neural Networks in this work}
\pagebreak

%-----------------------------------
%	SECTION 3
%-----------------------------------
\section{Word Embeddings Overview}
\subsection{Main Approches}
\subsubsection{The Skip-Gram Model (Word2Vec)}
\subsubsection{Global Vectors for Word Representation (GLoVe)}
\subsubsection{Syntactic Embeddings}
\subsubsection{FastText?}
\subsection{Contextualized Embeddings}
\subsubsection{ELMo}
\subsection{Multi-Lingual Word Embeddings}
\subsubsection{MUSE}
\subsection{Learning via Word Embeddings}
\subsubsection{*Basic usage with a FF-Network}
\subsubsection{*Updating embeddings through training}
\subsubsection{*Contextutalized representations with RNNs}
\subsection{Handling Out-Of-Vocabulary words}
\subsubsection{*By stemming}
\subsubsection{*By learning an internal representation}
\subsubsection{*Character-based embeddings}
\subsection{Use of Word Embeddings in this work}
\pagebreak


%-----------------------------------
%	SECTION 4
%-----------------------------------
\section{Natural Language Parsing and the Prepositional Phrase Attachment Problem}
\subsubsection{*Outline}
\subsection{Sentence Parsing}
\subsubsection{Dependency vs. Constituency Based Parsing}
\subsubsection{Syntactic vs. Semantic Parsers}
\subsection{The PP-Attachment problem}
\subsubsection{*Overview, examples}
\subsection{PP Attachment Disambiguation Approaches}
\subsubsection{As a sub-task of sentence parsing}
\subsubsection{As an isolated task}
\subsection{PP Attachment and PSS}
\subsubsection{*Motivation of using PSS for PPA}
\pagebreak

%-----------------------------------
%	SECTION 5
%-----------------------------------
\section{PPA work by Belinkov et. al 2014}
\subsubsection{*Overview, relevance to this work}
\subsubsection{*Intro to Belinkov work}
\subsubsection{* -- Isolated PPA models}
\subsubsection{* -- Incorporated to an existing parser}
\subsubsection{* -- Model architecture in brief}
\subsection{Datasets}
\subsubsection{*Structure, size and other notable stats}
\subsubsection{*Extraction process}
\subsection{Model}
\subsubsection{*Model framework}
\subsubsection{*Score function variations}
\subsubsection{The HPCD Variation}
\subsection{Word Vector Representation}
\subsubsection{*Word Embedding Variations}
\subsubsection{*Vector Enrichment (WordNet, VerbNet, etc)}
\subsubsection{*Full Vector Outline}
\subsection{Training}
\subsection{Results}
\subsection{Conclusion}
\subsubsection{*Brief summary and chapter 3 outline}
\pagebreak

%-----------------------------------
%	SECTION 6
%-----------------------------------
\section{Universal Dependencies}
\pagebreak

%-----------------------------------
%	SECTION 7
%-----------------------------------
\section{Named Entity Recognition}
