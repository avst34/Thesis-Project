% Chapter Template

\chapter{Background} % Main chapter title

\label{Background} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


%-----------------------------------
%	SECTION 1
%-----------------------------------
\section{Introduction to Preposition Supersenses}
\subsubsection{*Adpositions signal semantic relations}
\subsubsection{*These relations are ambiguous}
\subsubsection{*Disambiguations could be beneficial to downstream tasks}
\subsection{The SNACS Scheme}
\subsubsection{SNACS Label Hierarchy}
\subsubsection{Construal Analysis}
\subsection{The STREUSLE corpus}
\subsection{Previous Approaches}
\subsection{PSS in This Work}
\pagebreak

%-----------------------------------
%	SECTION 2
%-----------------------------------
\section{Neural Networks for NLP}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{Electron}
  \caption{ 
    (i) A single neuron with N inputs. \\
    (ii) A feed-forward network with a single output neuron \\
    (ii) A multiclass network architecture: A feed-forward network with a Softmax output layer of $l$ neurons \\    
  }
  \label{fig:networks}
\end{figure}

Inspired by the neural networks of the biological brain, Artificial Neural Networks are a family of computational models widely used today across many different fields of Artificial Intelligence research. Neural-based machine learning models have consistently been able to achieve and outperform state-of-the-art results when obtained using previous approaches. In the past decade, they have been a key aspect of several breakthroughs in computer vision and speech recognition, which led to a growing interest and popularity among other research fields including Natural Language Processing. This section lays out the foundations required for understanding the use of neural networks throughout this work.

\medskip

A single neuron is the basic element of a neural network. Similarly to a biological neuron in the human brain, it emits a real-valued signal based on input signals it receives from other neurons it is connected to. The emitted signal could be fed into other neurons at an upper layer, and so on, thus forming a network. Input signals emitted into the bottom layer of neurons are considered the input of the network (image pixels, sound wave amplitudes, etc). Similarly, the outputs emitted from the top-most layer are considered the output of the network (class probabilities, regression value, etc). Typically, each neuron represents some detectable feature of the input. The strength of the emitted signal represents the certainty of the detected feature, or lack thereof. The hierarchical structure of the network allows low-level features, detected by neurons at lower layers, to be used for detection of more complex features by neurons at higher layers, and so on.

\medskip

\subsection{A Single Neuron}

The output of a single neuron (figure \ref{fig:networks} (i)) is a composition of a weighted sum of its inputs (and perhaps a bias term), followed by a non-linearity - the Activation Function. Equation \ref{eq:neuron} provides a formal definition of the neuron function:
\begin{equation}
y = \sigma(\vec{w}\cdot\vec{x} + b)
\label{eq:neuron}
\end{equation}

Where \(\vec{x}\) is a vector of inputs, \(\vec{w}\) is a vector of weights, \(b\) is a bias term and \(\sigma\) is a non-linearity. Informally speaking, the weights determine which of the input signals the neuron is more sensitive to, and the bias term is used to adjust the weighted sum of the signals prior to the non-linearity. The use of non linear activation functions allows the network to model real world data, which is often complex and non linear. Figure \ref{fig:activations} describes two commonly used activation functions: $tanh$, and Rectified Linear Unit ($ReLU$). $tanh$ maps the input into the range of $(-1,1)$, which is useful for neurons detecting a binary feature, or networks used for binary classification. A neuron with the $ReLU$ activation will output a positive signal once the weighted sum of its inputs meets a certain threshold determined by the bias term, and zero otherwise. In other words, a $ReLU$ neuron will only "fire" if its input signals are strong enough. 

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{Electron}
  \caption{
  Commonly used activation functions \\
  (i) $ ReLU(z) = max(0, z) $\\ 
  (ii) $ tanh(z) = \frac{2}{1 + e^{-2x}} - 1 $ \\
  }
  \label{fig:activations}
\end{figure}

\subsection{Feed-forward networks}

A set of \(k\) neurons operating on the same vector of input signals and sharing the same activation function \(\{y_i = \sigma(\vec{w_i}\cdot\vec{x} + b_i)\}\), is considered a single layer. Following eq. \ref{eq:neuron}, such layer computes an affine transformation on a vector of \(n\) input signals  \(\vec{x}_{nx1}\), followed by an element-wise non-linearity, as described in equation \ref{eq:neuronlayer}. 

\begin{equation}
    \vec{y}_{kx1} = \vec{\sigma}({W_{kxn}\vec{x}_{nx1}} + \vec{b}_{kx1})
    \label{eq:neuronlayer}
\end{equation}

Where the \(i\)-th row of \(W_{kxn}\) and \(\vec{b}_{kx1}\) are respectively the weight vector and the bias term of the \(i\)-th neuron, and \(\vec{\sigma}\) performs element-wise computation of the activation function \(\sigma\). Layers of neurons can be stacked on top of each other by setting the output of one layer to be the input to the next, as demonstrated in eq. \ref{eq:feedforward}:
\begin{equation}
    \begin{split}
    \vec{y}^1_{kx1} = \vec{\sigma}^1({W^1_{kxn}\vec{x}_{nx1}} + \vec{b}^1_{kx1}) \\
    \vec{y}^2_{kx1} = \vec{\sigma}^2({W^2_{kxk}\vec{y}^1_{kx1}} + \vec{b}^2_{kx1}) \\
    \vec{y}^3_{kx1} = \vec{\sigma}^3({W^3_{kxk}\vec{y}^2_{kx1}} + \vec{b}^3_{kx1}) \\
    out = \vec{\sigma}^{out}({W^{out}_{1xk}\vec{y}^3_{kx1}} + b^{out})
    \end{split}
    \label{eq:feedforward}
\end{equation}

Eq. \ref{eq:feedforward} describes a network with an input layer of size \(n\) followed by three layers of size \(k\) and an output layer consisting of a single neuron. Each layer has its own set of weights, bias terms and activation function. Neural networks consisting of such architecture - i.e a sequence of neuron layers stacked on top of each other, are known as \textbf{Feed Forward Networks}, or \textbf{Multi Layered Perceptrons} (due to the similarity of the early Perceptron model and a neuron). In feed-forward networks, information flows in one direction starting from the input layer, through intermediate layers (a.k.a hidden layers) and the output layer. This is in contrast to Recurrent Neural Networks (described in section todo), where neuron connections could form a cycle. A feed-forward network with a structure similar to that which defined in eq. \ref{eq:feedforward} is shown in figure \ref{fig:networks} (ii).

\subsection{The Softmax Layer}
A Softmax layer is a neuron layer that uses the Softmax activation function, which squashes the outputs of the layer to form a probability distribution. It is mostly used in the common case of multiclass classification, where each sample is to be assigned with a label from a predefined set of labels. Assuming a label set of size $L$, a Softmax layer consisting of $L$ neurons will generate a probability estimation for each label in the set. In such scenario, this layer would be the output layer of the network.

The Softmax function is different from the previously mentioned activation functions, in the sense that it operates on an entire vector of signals rather than on each individual signal independently. It receives a vector of the (intermediate) outputs of a neuron layer as input, and it outputs a vector of probabilities of the same dimension. The Softmax activation function is defined as follows:

\begin{equation}
    Softmax( \vec{x} )_j = \frac{e^{\vec{x}_j}}{\sum_{i=1}^k e^{\vec{x}_i}}
    \label{eq:softmax}
\end{equation}

Where $x \in \mathbb{R}^k$ is a vector of input signals. The equation of a Softmax \emph{layer} is obtained by using Softmax as the activation function \(\vec{\sigma}\) of eq.  \ref{eq:neuronlayer}. A full outline of a multiclass network architecture using a Softmax layer is outlined in figure \ref{fig:networks} (iii).

\subsection{Training a Neural Network}
Given a neural network architecture, the process of training the network aims to find a good set of \emph{model parameters} for a particular task. The parameters of a network are defined to be the entire set of weights and biases of neurons in the network. For instance, the set of parameters of the network defined in eq. \ref{eq:feedforward} is $\theta = \{W^1, \vec{b^1}, W^2, \vec{b^2}, W^3, \vec{b^3}, W^{out}, \vec{b^{out}}\}$. We can therefore consider $\theta$ as a vector of real-valued parameters by ordering and flattening the matrices and vectors in the set. Then, a neural network can be thought of as a function of an input sample $\vec{x}$ and a vector of model parameters $\theta$: 
$$ \vec{y} = \textbf{f}(\vec{x}, \theta) $$ 
In the supervised setting, we have a set of $N$ training samples $T = \{(\vec{x_i}, \vec{y_i})\}$ and we wish to obtain the best parameter vector $\hat{\theta}$ such that $\textbf{f}(\vec{x_i}, \hat{\theta})$ would be a good predictor of $y_i$ on average across all samples $(\vec{x_i}, \vec{y_i}) \in T$.  We use a \emph{loss function} $\mathcal{L}$ to measure the inconsistency between the ground truth $\vec{y_i}$ and the prediction $\textbf{f}(\vec{x_i}, \theta)$, which is also known as the \emph{cost} or \emph{loss} of the prediction. The learning process aims to minimize the average loss on the entire training set\footnote{In some cases, a regularization term $R(\theta)$ is added to the loss function as a mean to prevent overfitting. This is discussed in section todo.}:

\begin{equation}
\hat{\theta} = \argmin_{\theta} \frac{1}{N}\sum_{i=1}^N \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), y_i)
\label{eq:bestparams}    
\end{equation}

\subsubsection{Stochastic Gradient Descent with Backpropagation}
While computing an exact solution for eq \ref{eq:bestparams} is often intractable, there are many optimizers aiming to find a good estimation of the best parameter vector $\hat{\theta}$. Most of these optimizers are gradient-based, and are typically variations of the Gradient Descent (GD) minimization algorithm. GD is an iterative process aiming to minimize a differentiable function $t$. It begins with a randomly assigned $\theta$. In each iteration, the gradient $\nabla t = \frac{\partial t}{\partial \theta}$ is computed, and $\theta$ is updated accordingly:

$$ \theta \leftarrow \theta - \textit{lr} \cdot \nabla t(\theta) $$

This update step pushes $\theta$ in the opposite direction of the gradient, thus aiming to decrease $t(\theta)$. This process is repeated until a local minima is reached, a state which is practically detected by observing that $t(\theta)$ has not significantly decreased in the last window of iterations. The hyperparameter \textit{lr} - the \textit{learning rate} - controls the magnitude of the update step. A small learning rate may result in slow convergence, while a big learning rate may prevent the process from converging at all (as the magnified gradient could push $\theta$ too far and actually \emph{increase} $t(\theta)$). In some variations of GD, the learning rate is multiplied by a decay factor after each predefined number of iterations. 

Following eq \ref{eq:bestparams}, the target function we wish to minimize in order to train the network is:

$$ t(\theta) = \frac{1}{N}\sum_{i=1}^N \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), y_i) $$

For $t$ to be differentiable with regards to $\theta$, we require choosing a neural network $f$ and a loss function $\mathcal{L}$ such that $f$ is differentiable with regards to $\theta$, and $\mathcal{L}$ is differentiable with regards to its first argument - the predicted vector.

In practice, computing the gradient of $t$ is very computationally expensive for a large training set, as it requires computing the gradient of the loss of each training sample separately. Instead, in a process known as \textbf{Mini-Batch Gradient Descent} or \textbf{Stochastic Gradient Descent (SGD)}, an approximation of the gradient is computed. The set of input samples is divided into batches of size $BATCH\_SIZE$. At each step of the iteration, the gradient is computed on the average loss of only the next batch of samples:

$$ \hat{t}(\theta) = \frac{1}{BATCH\_SIZE}\sum_{(\vec{x_i}, \vec{y_i}) \in Batch} \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), \vec{y_i})  $$

$$ \theta \leftarrow \theta - \textit{lr} \cdot \nabla \hat{t}(\theta) $$

This update step can be thought of as training the network on a batch of samples. An \emph{epoch} represents a full pass of training over the entire set of batches in the training set. It is very common to train a network through several epochs, while shuffling the samples in between. The amount of epochs depends on the convergence rate and the size of the training set.

The following are a few commonly used variations of SGD:
\begin{itemize}
    \item \textbf{Adaptive Gradient Algorithm (AdaGrad)}: ???todo???
    \item \textbf{Root Mean Square Propagation (RMSProp)}: ???todo???
    \item \textbf{Adaptive Moment estimation (Adam}: ???todo???    
\end{itemize}

SGD (along with its variations) is a heuristic process and is not guaranteed to converge to a global minima. Emprical results consistently show, however, that neural networks can be trained successfully even when using plain SGD.

The gradient for a single sample in the training set -  $\frac{\partial \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), \vec{y_i})}{\partial \theta}$ - can be computed efficiently in a process known as \textbf{Backpropagation}. The Backpropagation
algorithm is able to efficiently compute the gradient of a real function $t(\theta)$, provided that it holds several key properties: (a) $t$ is a composition of real functions, (b) each function takes only elements from $\theta$ or an output of another function as inputs, and (c) the partial derivatives of each function can be computed efficiently.  

It follows from (a), (b) that $t$ can be described as a directed acyclic graph, where each graph node $\textbf{v}$ represents an element from $\theta$ (a \textit{$\theta$-node}), or an output of an intermediate function (where the function and output are notated as $\textbf{f}_{\textbf{v}}$ and $o_{\textbf{v}}$ respectively). An edge $\textbf{u}\rightarrow\textbf{v}$ is present when $o_{\textbf{u}}$ is an input of $\textbf{f}_\textbf{v}$. It holds for the (single) sink node $\textbf{s}$ that $o_{\textbf{s}} = t(\theta)$. Nodes representing elements from $\theta$  are the only ones with no incoming edges. 

The Backpropagation process computes a partial derivative value of $t$ for each \emph{edge} in the graph. It begins with edges connected to the sink node, and going backwards in reversed topological order (utilizing the chain rule for function derivation). The partial derivative of each element in $\theta$ is then the sum of partial derivatives over all outgoing edges from the corresponding element node.

The following briefly outlines a single Backpropagation step computing the partial derivative for a single edge $\textbf{u}\rightarrow\textbf{v}$ (notated as $\delta_{\textbf{u}\rightarrow\textbf{v}}$):   

\begin{equation}
    \delta_{\textbf{u}\rightarrow\textbf{v}} = \frac{\partial o_{\textbf{v}}}{\partial o_{\textbf{u}}} \cdot
    \begin{cases}
     \sum\limits_{e \in OutEdges(\textbf{v})} \delta_{e}     & \text{if $OutEdges(\textbf{v}) \neq \emptyset$}\\
    1       & \text{otherwise}
    \end{cases}
    \label{backprop}
\end{equation}

Where $OutEdges(\textbf{v})$ is the set out all outgoing edges from node $\textbf{v}$. The partial derivative of $t(\theta)$ w.r.t $\theta_i$,  where $\textbf{w}$ is the corresponding $\theta$\textit{-node}, is then:

$$\frac{\partial t(\theta)}{\partial \theta_i} = \sum\limits_{e \in OutEdges(\textbf{w})} \delta_{e} $$ 

Backpropagation thus can be used to compute the gradient for the loss of a given training sample in a feed-forward network, provided that the activation functions and loss function are easy to differentiate. The process of computing the gradient on a 3-layered network is described in figure \ref{fig:backprop}

\begin{figure}
    \centering
    \includegraphics{Figures/Electron.pdf}
    \caption{Backpropagation demonstrated on a 3-layered feed forward neural network}
    \label{fig:backprop}
\end{figure}


\subsubsection{*Neural nets overview}
\subsubsection{*Activation functions}
\subsubsection{*Loss functions}
\subsubsection{*Training}
\subsubsection{*Hyperparameters vs. Model parameters, tuning}
\subsubsection{*Regularization and drop-out}
\subsection{Feed Forward Networks}
\subsection{Recurrent Neural Networks}
\subsubsection{Bidirectional RNN}
\subsubsection{Stacked RNNs}
\subsubsection{Long-Short-Term-Memory (LSTM)}
\subsection{Lookup Tables}
\subsection{The Stacked BiLSTM-MLP-Softmax Architecture}
\subsection{Neural Networks in this work}
\pagebreak

%-----------------------------------
%	SECTION 3
%-----------------------------------
\section{Word Embeddings Overview}
\subsection{Main Approches}
\subsubsection{The Skip-Gram Model (Word2Vec)}
\subsubsection{Global Vectors for Word Representation (GLoVe)}
\subsubsection{Syntactic Embeddings}
\subsubsection{FastText?}
\subsection{Contextualized Embeddings}
\subsubsection{ELMo}
\subsection{Multi-Lingual Word Embeddings}
\subsubsection{MUSE}
\subsection{Learning via Word Embeddings}
\subsubsection{*Basic usage with a FF-Network}
\subsubsection{*Updating embeddings through training}
\subsubsection{*Contextutalized representations with RNNs}
\subsection{Handling Out-Of-Vocabulary words}
\subsubsection{*By stemming}
\subsubsection{*By learning an internal representation}
\subsubsection{*Character-based embeddings}
\subsection{Use of Word Embeddings in this work}
\pagebreak


%-----------------------------------
%	SECTION 4
%-----------------------------------
\section{Natural Language Parsing and the Prepositional Phrase Attachment Problem}
\subsubsection{*Outline}
\subsection{Sentence Parsing}
\subsubsection{Dependency vs. Constituency Based Parsing}
\subsubsection{Syntactic vs. Semantic Parsers}
\subsection{The PP-Attachment problem}
\subsubsection{*Overview, examples}
\subsection{PP Attachment Disambiguation Approaches}
\subsubsection{As a sub-task of sentence parsing}
\subsubsection{As an isolated task}
\subsection{PP Attachment and PSS}
\subsubsection{*Motivation of using PSS for PPA}
\pagebreak

%-----------------------------------
%	SECTION 5
%-----------------------------------

%-----------------------------------
%	SECTION 6
%-----------------------------------
\section{Universal Dependencies}
\pagebreak

%-----------------------------------
%	SECTION 7
%-----------------------------------
\section{Named Entity Recognition}
