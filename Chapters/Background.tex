% Chapter Template

\chapter{Background} % Main chapter title

\label{Background} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


%-----------------------------------
%	SECTION 1
%-----------------------------------
\section{Introduction to Preposition Supersenses}
Function words such as adpositions and possessives have an important role in a language - though bearing little semantic content, they provide means to convey semantic relations between different parts of a sentence. Prepositions, for example, can serve a variety of semantic relations: location (\textit{I am waiting \textbf{at} the bus stop}), time (\textit{I've been waiting \textbf{for} an hour}), possession (\textit{The passenger \textbf{with} the backpack}), quantity (\textit{There are dozens \textbf{of} people with me}), membership (\textit{The singer \textbf{in} the band}) and so on.

The nature of such relations is highly ambiguous with regards to the preposition itself, as most prepositions can be used in a variety of ways, each yielding a different kind of semantic relation. For instance, consider the use of the preposition “in” in the following sentences: 

\begin{enumerate}
    \item The client will meet you \textbf{in} 10 minutes.
    \item The client will meet you \textbf{in} your office.
\end{enumerate}

In the former, \emph{in} is used to convey a temporal relation, while in the latter the relation is of a locative nature.  As this example demonstrates, the \emph{object} of the preposition (\emph{10 minutes/your office}) is in many cases a key distinguisher between different relations, especially for the more frequent prepositions. The same  holds for the \emph{governer} of the prepositional phrase - \textit{I \textbf{asked/payed} for assistance}. Possessives are similarly ambiguous: \emph{DiCaprio’s \textbf{nominations/movies}}. 

In fact, most prepositions are ambiguous in these aspects. In order to resolve this ambiguity, one has to examine not only the preposition itself, but also the context of the preposition. Such a disambiguation, which assigns each preposition with its correct semantic relation, could provide another layer of information about the meaning of the sentence. That information may be useful for downstream NLP tasks.

\subsection{The SNACS Scheme} \label{sec:snacsscheme}

\begin{figure}
    \centering
    \includegraphics{Figures/Electron.pdf}
    \caption{The SNACS scheme}
    \label{fig:snacs}
\end{figure}

A recent class-based approach for characterizing such semantic relations, which this work focuses on, is \textbf{Semantic Network of Adposition and Case Supersense} (SNACS) (todo: reference) by Schneider et al 2017. SNACS covers adpositions, cases and a broad range of preposition-like phrases, such as: multiword prepositions (e.g., \emph{out of} , \emph{in front of}), intransitive particles (\emph{He flew \textbf{away}}), purpose infinitive clauses (\emph{Open the door \textbf{to} let in some air}), prepositions with clausal complements (\emph{It rained \textbf{before} the party started}), and idiomatic prepositional phrases (\textit{at large}). It consists of an hierarchical inventory of labels (which from here on will be referred to as preposition supersenses or PSS), representing different semantic relations (See figure \ref{fig:snacs}). The SNACS hierarchy aims to cover a broad range of semantic ideas on the one hand, but on the other doing so using a relatively small and annotator-friendly set of supersenses.

\subsubsection{SNACS Label Hierarchy}

The SNACS hierarchy is comprised of 50 supersenses at 4 levels of depth. The top-level categories are:
\begin{itemize}
    \item CIRCUMSTANCE :  Circumstantial information, usually non-core properties of events (e.g., location, time, means, purpose)
    \item PARTICIPANT : Entity playing a role in an event
    \item CONFIGURATION : Thing, usually an entity or property, involved in a static relationship to some other entity
\end{itemize}

The 3 subtrees loosely parallel adverbial adjuncts, event arguments, and adnominal complements, respectively. The PARTICIPANT and CIRCUMSTANCE subtrees primarily reflect semantic relationships prototypical to verbal arguments/adjuncts. Many CIRCUMSTANCE subtypes, like LOCUS (the concrete or abstract location of something), can be governed by eventive and non-eventive nominals as well as verbs: \emph{eat \textbf{in} the restaurant, a party \textbf{in} the restaurant, a table \textbf{in} the restaurant}. CONFIGURATION mainly encompasses non-spatiotemporal relations holding between entities, such as quantity, possession, and part/whole. SNACS does not use multiple inheritance, so there is no overlap between the 3 regions. The supersenses can be understood as roles in fundamental types of scenes (or schemas) such as: 

\begin{itemize}
\item LOCATION - THEME is located at LOCUS;
\item MOTION - THEME moves from SOURCE along PATH to GOAL ;
\item TRANSITIVE ACTION - AGENT acts on THEME , perhaps using an INSTRUMENT ;
\item POSSESSION - POSSESSION belongs to POSSESSOR ;
\item TRANSFER - THEME changes possession from ORIGINATOR to RECIPIENT , perhaps with COST;
\item PERCEPTION - EXPERIENCER is mentally affected by STIMULUS; 
\item COGNITION - EXPERIENCER contemplates TOPIC;
\item COMMUNICATION - information (TOPIC) flows from ORIGINATOR to RECIPIENT, perhaps via an INSTRUMENT. 
\end{itemize}

For AGENT , CO-AGENT , EXPERIENCER , ORIGINATOR , RECIPIENT, BENEFICIARY , POSSESSOR , and SOCIAL REL, the object of the preposition is prototypically animate. Because prepositions and possessives cover a vast swath of semantic space, the limit of 50 categories requires addressing a great many nonprototypical, borderline, and special cases, yielding a 75-page annotation manual with over 400 example sentences ( Schneider et al. , 2018). SNACS can be considered as an refinement of Universal Semantic Tagset (Abzianidze and Bos , 2017) - a crosslinguistic inventory of semantic classes for content and function words. In their specification, prepositions and possessives are simply tagged with REL, which does not disambiguate the nature of the relational meaning. SNACS supersenses categories can thus be understood as refinements to REL.

\subsubsection{Construal Analysis} \label{sec:construal}

The SNACS schema assigns an instance of an adposition with two supersenses: a role supersense and a function supersense (both from the SNACS hierarchy). The choice to use not one but two supersenses for each adposition, arises from the construal analysis performed by Hwang et al.(2017). They shows how in some cases, the supersense suggested by the preposition and the verb (corresponding to the function supersense) would be different than the situation of the established scene (corresponding to the role supersense). For instance, consider the following sentences:

\begin{enumerate}
    \item Vernon works \textbf{at} Grunnings.
    \item Vernon works \textbf{for} Grunnings.
\end{enumerate}

The semantics of the scene in (1, 2) is the same: it is an employment relationship, and the PP contains the employer. SNACS has the label ORG ROLE for this purpose. At the same time, \textbf{at} in (2) strongly suggests a locational relationship, which would correspond to the label LOCUS; consistent with this hypothesis, Where does Vernon work? is a perfectly good way to ask a question that could be answered by the prepositional phrase ‘at Grunings’. In this example, then, there is an overlap between locational meaning and organizational-belonging meaning. (2) is similar except the \textbf{for} suggests a notion of BENEFICIARY: the employee is working on behalf of the employer. Following Hwang et al.(2017), SNACS chose to handle this overlap by assigning not one but two supersenses (using the same inventory) for each preposition:

\begin{itemize}
    \item Function supersense - representing the lexical semantic contribution of the preposition.
    \item Role supersense - representing the semantic role or relation mediated by the preposition.
\end{itemize}

The innovative claim is that, in addition to a preposition’s relationship with its head, the prepositional choice introduces another layer of meaning or \textbf{construal} that brings additional nuance, creating the difficulty we see in the annotation of (1,2). Following the ROLE->FUNCTION notation, (1) would be annotated as ORG ROLE->LOCUS and (2) as ORG-ROLE->BENEFICIARY to expose their common truth-semantic meaning but slightly different portrayals owing to the different prepositions.

Another useful application of the construal analysis is with the verb \textbf{\emph{put}}, which can combine with any locative PP to express a destination:

\begin{itemize}
    \item[(6)] Put it on / by / behind / on top of /. . . the door. 
    
    GOAL->LOCUS 
\end{itemize}


I.e., the preposition signals a LOCUS, but the door serves as the GOAL with respect to the scene. This approach also allows for resolution of various semantic phenomena including perceptual scenes (e.g., \emph{I care \textbf{about} education}), where \textbf{about} is both the topic of cogitation and perceptual stimulus of caring: STIMULUS->TOPIC ), and fictive motion (Talmy , 1996), where static location is described using motion verbiage (as in \emph{The road runs \textbf{through} the forest}: LOCUS->PATH). 

\subsection{The STREUSLE corpus}

The SNACS annotation scheme was manually applied to adpositions in the STREUSLE corpus, a collection of online consumer reviews taken from the English Web Treebank (todo: section reference). Figure ?? shows some real examples from the STREUSLE corpus. Note that prepositions in (1) are annotated with a single supersense, which is the notation for cases where both ROLE and FUNCTION are described using the same supersense. In fact, as shown in ??, this is the common case.

\subsection{Previous Approaches}

Several attempts have been made in characterizing these semantic relations. Studies of preposition semantics in linguistics and cognitive science have generally focused on the domains of space and time (e.g., Herskovits , 1986 ; Bowerman and Choi , 2001 ; Regier , 1996 ; Khetarpal et al. , 2009 ; Xu and Kemp , 2010 ; Zwarts and Winter , 2000 ) or on motivated polysemy structures that cover additional meanings beyond core spatial senses (Brugman , 1981 ; Lakoff , 1987 ; Tyler and Evans , 2003 ; Lindstromberg , 2010). Possessive constructions can likewise denote a number of semantic relations, and various factors - including semantics - influence whether attributive possession in English will be expressed with of , or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor , 1996 ; Nikiforidou , 1991 ; Rosenbach , 2002 ; Heine , 2006 ; Wolk et al. , 2013 ; Shih et al. , 2015 ). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives falls into two categories: the lexicographic/word sense disambiguation approach ( Litkowski and Hargraves , 2005 , 2007 ; Litkowski , 2014 ; Ye and Baldwin , 2007 ; Saint-Dizier , 2006 ; Dahlmeier et al. , 2009 ; Tratz and Hovy , 2009 ; Hovy et al. , 2010 , 2011 ; Tratz and Hovy , 2013 ), and the semantic class approach ( Moldovan et al. , 2004 ; Badulescu and Moldovan , 2009 ; O’Hara and Wiebe , 2009 ; Srikumar and Roth , 2011 , 2013 ; Schneider et al. , 2015 , 2016 ; Hwang et al. , 2017 , see also Müller et al. , 2012 for German).  The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to generalize more easily to new types and usages.


\subsection{PSS in This Work}

In chapter ?? we present a neural model for SNACS PSS disambiguation, trained and evaluated on the STREUSLE corpus over several settings. In chapter ?? we examine how beneficial PSS information could be to the PP-Attachment problem, by adding it as a new feature to an existing PP-Attachment disambiguation model. (todo chapter 3) Chapter ?? deals with the relation between ROLE and FUNCTION (which tend to collide), by evaluating models trained to disambiguate one by using the other (along with the preposition and its context).

\pagebreak

%-----------------------------------
%	SECTION 2
%-----------------------------------
\section{Neural Networks for NLP}

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{Electron}
  \caption{ 
    (i) A single neuron with N inputs. \\
    (ii) A feed-forward network with a single output neuron \\
    (ii) A multiclass network architecture: A feed-forward network with a Softmax output layer of $l$ neurons \\    
  }
  \label{fig:networks}
\end{figure}

Inspired by the neural networks of the biological brain, Artificial Neural Networks are a family of computational models widely used today across many different fields of Artificial Intelligence research. Neural-based machine learning models have consistently been able to achieve and outperform state-of-the-art results when obtained using previous approaches. In the past decade, they have been a key aspect of several breakthroughs in computer vision and speech recognition, which led to a growing interest and popularity among other research fields including Natural Language Processing. This section lays out the foundations required for understanding the use of neural networks throughout this work.

\medskip

A single neuron is the basic element of a neural network. Similarly to a biological neuron in the human brain, it emits a real-valued signal based on input signals it receives from other neurons it is connected to. The emitted signal could be fed into other neurons at an upper layer, and so on, thus forming a network. Input signals emitted into the bottom layer of neurons are considered the input of the network (image pixels, sound wave amplitudes, etc). Similarly, the outputs emitted from the top-most layer are considered the output of the network (class probabilities, regression value, etc). Typically, each neuron represents some detectable feature of the input. The strength of the emitted signal represents the certainty of the detected feature, or lack thereof. The hierarchical structure of the network allows low-level features, detected by neurons at lower layers, to be used for detection of more complex features by neurons at higher layers, and so on.

\medskip

\subsection{A Single Neuron}

The output of a single neuron (figure \ref{fig:networks} (i)) is a composition of a weighted sum of its inputs (and perhaps a bias term), followed by a non-linearity - the Activation Function. Equation \ref{eq:neuron} provides a formal definition of the neuron function:
\begin{equation}
y = \sigma(\vec{w}\cdot\vec{x} + b)
\label{eq:neuron}
\end{equation}

Where \(\vec{x}\) is a vector of inputs, \(\vec{w}\) is a vector of weights, \(b\) is a bias term and \(\sigma\) is a non-linearity. Informally speaking, the weights determine which of the input signals the neuron is more sensitive to, and the bias term is used to adjust the weighted sum of the signals prior to the non-linearity. The use of non linear activation functions allows the network to model real world data, which is often complex and non linear. Figure \ref{fig:activations} describes two commonly used activation functions: $tanh$, and Rectified Linear Unit ($ReLU$). $tanh$ maps the input into the range of $(-1,1)$, which is useful for neurons detecting a binary feature, or networks used for binary classification. A neuron with the $ReLU$ activation will output a positive signal once the weighted sum of its inputs meets a certain threshold determined by the bias term, and zero otherwise. In other words, a $ReLU$ neuron will only "fire" if its input signals are strong enough. 

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{Electron}
  \caption{
  Commonly used activation functions \\
  (i) $ ReLU(z) = max(0, z) $\\ 
  (ii) $ tanh(z) = \frac{2}{1 + e^{-2x}} - 1 $ \\
  }
  \label{fig:activations}
\end{figure}

\subsection{Feed-forward networks}

A set of \(k\) neurons operating on the same vector of input signals and sharing the same activation function \(\{y_i = \sigma(\vec{w_i}\cdot\vec{x} + b_i)\}\), is considered a single layer. Following eq. \ref{eq:neuron}, such layer computes an affine transformation on a vector of \(n\) input signals  \(\vec{x}_{nx1}\), followed by an element-wise non-linearity, as described in equation \ref{eq:neuronlayer}. 

\begin{equation}
    \vec{y}_{kx1} = \vec{\sigma}({W_{kxn}\vec{x}_{nx1}} + \vec{b}_{kx1})
    \label{eq:neuronlayer}
\end{equation}

Where the \(i\)-th row of \(W_{kxn}\) and \(\vec{b}_{kx1}\) are respectively the weight vector and the bias term of the \(i\)-th neuron, and \(\vec{\sigma}\) performs element-wise computation of the activation function \(\sigma\). Layers of neurons can be stacked on top of each other by setting the output of one layer to be the input to the next, as demonstrated in eq. \ref{eq:feedforward}:
\begin{equation}
    \begin{split}
    \vec{y}^1_{kx1} = \vec{\sigma}^1({W^1_{kxn}\vec{x}_{nx1}} + \vec{b}^1_{kx1}) \\
    \vec{y}^2_{kx1} = \vec{\sigma}^2({W^2_{kxk}\vec{y}^1_{kx1}} + \vec{b}^2_{kx1}) \\
    \vec{y}^3_{kx1} = \vec{\sigma}^3({W^3_{kxk}\vec{y}^2_{kx1}} + \vec{b}^3_{kx1}) \\
    out = \vec{\sigma}^{out}({W^{out}_{1xk}\vec{y}^3_{kx1}} + b^{out})
    \end{split}
    \label{eq:feedforward}
\end{equation}

Eq. \ref{eq:feedforward} describes a network with an input layer of size \(n\) followed by three layers of size \(k\) and an output layer consisting of a single neuron. Each layer has its own set of weights, bias terms and activation function. Neural networks consisting of such architecture - i.e a sequence of neuron layers stacked on top of each other, are known as \textbf{Feed Forward Networks}, or \textbf{Multi Layered Perceptrons} (due to the similarity of the early Perceptron model and a neuron). In feed-forward networks, information flows in one direction starting from the input layer, through intermediate layers (a.k.a hidden layers) and the output layer. This is in contrast to Recurrent Neural Networks (described in section todo), where neuron connections could form a cycle. A feed-forward network with a structure similar to that which defined in eq. \ref{eq:feedforward} is shown in figure \ref{fig:networks} (ii).

\subsection{The Softmax Layer}
A Softmax layer is a neuron layer that uses the Softmax activation function, which squashes the outputs of the layer to form a probability distribution. It is mostly used in the common case of multiclass classification, where each sample is to be assigned with a label from a predefined set of labels. Assuming a label set of size $L$, a Softmax layer consisting of $L$ neurons will generate a probability estimation for each label in the set. In such scenario, this layer would be the output layer of the network.

The Softmax function is different from the previously mentioned activation functions, in the sense that it operates on an entire vector of signals rather than on each individual signal independently. It receives a vector of the (intermediate) outputs of a neuron layer as input, and it outputs a vector of probabilities of the same dimension. The Softmax activation function is defined as follows:

\begin{equation}
    Softmax( \vec{x} )_j = \frac{e^{\vec{x}_j}}{\sum_{i=1}^k e^{\vec{x}_i}}
    \label{eq:softmax}
\end{equation}

Where $x \in \mathbb{R}^k$ is a vector of input signals. The equation of a Softmax \emph{layer} is obtained by using Softmax as the activation function \(\vec{\sigma}\) of eq.  \ref{eq:neuronlayer}. A full outline of a multiclass network architecture using a Softmax layer is outlined in figure \ref{fig:networks} (iii).

\subsection{Loss Functions}

A loss function $\mathcal{L}(\hat{y}, y)$ computes a (usually non-negative) value, measuring the inconsistency between a prediction $\hat{y}$ and ground truth $y$ for a given sample. 
As this work deals mostly with classification tasks, the following will describe two very common loss functions used for classification: the Cross Entropy loss, and the Hinge loss.

\subsubsection{Cross-Entropy Loss}
The Cross-Entropy loss function assumes the predicted vector $\hat{y}_{1xn}$ is a probability distribution, where $\hat{y}_i$ is the probability assigned to the $i$-th class from a set of $n$ classes. It is usually modeled by using Softmax as the output layer of the network. The ground truth $y$ is therefore a one-hot vector, with a probability of 1 assigned to the correct label (and 0 for the rest). The Cross-Entropy loss is then defined as the cross-entropy between the true distribution and the predicted one:

\begin{equation}
    \mathcal{L}_{CE}(\hat{y}, y) = \sum_{i=1}^{n} -y_{i}log(\hat{y}_i)
    \label{crossentropy}
\end{equation}

In the described setting, the Cross-Entropy boils down to simply taking the (minus) log of the probability assigned by the model to the correct label. It is worth noting, however, that Cross-Entropy loss can be used just as well for settings where the ground truth is not a single correct label, but a \emph{probability distribution} over the set of labels. 
A key property of Cross-Entropy loss is that it tends to infinity as the predicted probability tends to zero, rapidly penalizing the model for predictions that are both incorrect and confident.

\subsubsection{Hinge Loss}
The Hinge loss is suitable for scenarios where the model assigns each candidate with an unbounded score (rather than a probability), and the highest scoring candidate is considered as the predicted candidate. It takes a marginal approach, by penalizing predictions where the score assigned to the correct prediction is not larger than the rest by at least 1. The Hinge loss then corresponds to the score difference that violates this constraint mostly:

\begin{equation}
    \mathcal{L}_{Hinge}(\hat{y}, y) = max(0, 1 + max(\{\hat{y}_{i} | i \neq t\}) - \hat{y}_t)    \label{hingeloss}
\end{equation}

Where $t$ is the index of the correct candidate (obtained from $y$). Predictions assigning the correct candidate with the highest score, by a margin of at least 1, will receive a loss of 0. Otherwise, the loss is the violating difference:  $ 1 + \hat{y}_p - \hat{y}_t $ where $p$ is the index corresponding to the highest scoring candidate.

\subsection{Training a Neural Network}
Given a neural network architecture, the process of training the network aims to find a good set of \emph{model parameters} for a particular task. The parameters of a network are defined to be the entire set of weights and biases of neurons in the network. For instance, the set of parameters of the network defined in eq. \ref{eq:feedforward} is $\theta = \{W^1, \vec{b^1}, W^2, \vec{b^2}, W^3, \vec{b^3}, W^{out}, \vec{b^{out}}\}$. We can therefore consider $\theta$ as a vector of real-valued parameters by ordering and flattening the matrices and vectors in the set. Then, a neural network can be thought of as a function of an input sample $\vec{x}$ and a vector of model parameters $\theta$: 
$$ \vec{y} = \textbf{f}(\vec{x}, \theta) $$ 
In the supervised setting, we have a set of $N$ training samples $T = \{(\vec{x_i}, \vec{y_i})\}$ and we wish to obtain the best parameter vector $\hat{\theta}$ such that $\textbf{f}(\vec{x_i}, \hat{\theta})$ would be a good predictor of $y_i$ on average across all samples $(\vec{x_i}, \vec{y_i}) \in T$.  We use a loss function $\mathcal{L}$ to measure the inconsistency between the ground truth $\vec{y_i}$ and the prediction $\textbf{f}(\vec{x_i}, \theta)$, which is also known as the \emph{cost} or \emph{loss} of the prediction. The learning process aims to minimize the average loss on the entire training set\footnote{In some cases, a regularization term $R(\theta)$ is added to the loss function as a mean to prevent overfitting. This is discussed in section todo.}:

\begin{equation}
\hat{\theta} = \argmin_{\theta} \frac{1}{N}\sum_{i=1}^N \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), y_i)
\label{eq:bestparams}    
\end{equation}

\subsubsection{Stochastic Gradient Descent with Backpropagation}
While computing an exact solution for eq \ref{eq:bestparams} is often intractable, there are many optimizers aiming to find a good estimation of the best parameter vector $\hat{\theta}$. Most of these optimizers are gradient-based, and are typically variations of the Gradient Descent (GD) minimization algorithm. GD is an iterative process aiming to minimize a differentiable function $t$. It begins with a randomly assigned $\theta$. In each iteration, the gradient $\nabla t(\theta)$ is computed, and $\theta$ is updated accordingly:

$$ \theta \leftarrow \theta - \textit{lr} \cdot \nabla t(\theta) $$

This update step pushes $\theta$ in the opposite direction of the gradient, thus aiming to decrease $t(\theta)$. This process is repeated until a local minima is reached, a state which is practically detected by observing that $t(\theta)$ has not significantly decreased in the last window of iterations. The hyperparameter \textit{lr} - the \textit{learning rate} - controls the magnitude of the update step. A small learning rate may result in slow convergence, while a large learning rate may prevent the process from converging at all (as the magnified gradient could push $\theta$ too far and actually \emph{increase} $t(\theta)$). In some variations of GD, the learning rate is multiplied by a decay factor after each predefined number of iterations. 

Following eq \ref{eq:bestparams}, the target function we wish to minimize in order to train the network is:

$$ t(\theta) = \frac{1}{N}\sum_{i=1}^N \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), y_i) $$

For $t$ to be differentiable with regards to $\theta$, we require choosing a neural network $f$ and a loss function $\mathcal{L}$ such that $f$ is differentiable with regards to $\theta$, and $\mathcal{L}$ is differentiable with regards to its first argument - the predicted vector.

In practice, computing the gradient of $t$ is very computationally expensive for a large training set, as it requires computing the gradient of the loss of each training sample separately. Instead, in a process known as \textbf{Mini-Batch Gradient Descent} or \textbf{Stochastic Gradient Descent (SGD)}, an approximation of the gradient is computed. The set of input samples is divided into batches of size $BATCH\_SIZE$. At each step of the iteration, the gradient is computed on the average loss of only the next batch of samples:

$$ \hat{t}(\theta) = \frac{1}{BATCH\_SIZE}\sum_{(\vec{x_i}, \vec{y_i}) \in Batch} \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), \vec{y_i})  $$

$$ \theta \leftarrow \theta - \textit{lr} \cdot \nabla \hat{t}(\theta) $$

This update step can be thought of as training the network on a batch of samples. An \emph{epoch} represents a full pass of training over the entire set of batches in the training set. It is very common to train a network through several epochs, while shuffling the order of samples between each epoch. The right amount of epochs depends on the convergence rate and the size of the training set.

The following are a few commonly used variations of SGD:
\begin{itemize}
    \item \textbf{Adaptive Gradient Algorithm (AdaGrad)}: ???todo???
    \item \textbf{Root Mean Square Propagation (RMSProp)}: ???todo???
    \item \textbf{Adaptive Moment estimation (Adam}: ???todo???    
\end{itemize}

SGD (along with its variations) is a heuristic process and is not guaranteed to converge to a global minima. Emprical results consistently show, however, that neural networks can be trained successfully even when using plain SGD.

The gradient for a single sample in the training set -  $\nabla_\theta \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), \vec{y_i})$ - can be computed efficiently in a process known as \textbf{Backpropagation}. The Backpropagation
algorithm is able to efficiently compute the gradient of a real function $t(\theta)$, provided that it holds several key properties: (a) $t$ is a composition of real functions, (b) each function takes only elements from $\theta$ or an output of another function as inputs, and (c) the partial derivatives of each function can be computed efficiently.  

It follows from (a), (b) that $t$ can be described as a directed acyclic graph, where each graph node $\textbf{v}$ represents an element from $\theta$ (a \textit{$\theta$-node}), or an output of an intermediate function (where the function and output are notated as $\textbf{f}_{\textbf{v}}$ and $o_{\textbf{v}}$ respectively). An edge $\textbf{u}\rightarrow\textbf{v}$ is present when $o_{\textbf{u}}$ is an input of $\textbf{f}_\textbf{v}$. It holds for the (single) sink node $\textbf{s}$ that $o_{\textbf{s}} = t(\theta)$. The \textit{$\theta$-nodes} are the only ones with no incoming edges. 

The Backpropagation process computes a partial derivative value of $t$ for each \emph{edge} in the graph. It begins with edges connected to the sink node, and going backwards in reversed topological order (utilizing the chain rule for function derivation). The partial derivative of each element in $\theta$ is then the sum of partial derivatives over all outgoing edges from the corresponding \textit{$\theta$-node}.

The following briefly outlines a single Backpropagation step, i.e computing the partial derivative for a single edge $\textbf{u}\rightarrow\textbf{v}$ (notated as $\delta_{\textbf{u}\rightarrow\textbf{v}}$):   

\begin{equation}
    \delta_{\textbf{u}\rightarrow\textbf{v}} = \frac{\partial o_{\textbf{v}}}{\partial o_{\textbf{u}}} \cdot
    \begin{cases}
     \sum\limits_{e \in OutEdges(\textbf{v})} \delta_{e}     & \text{if $OutEdges(\textbf{v}) \neq \emptyset$}\\
    1       & \text{otherwise}
    \end{cases}
    \label{backprop}
\end{equation}

Where $OutEdges(\textbf{v})$ is the set out all outgoing edges from node $\textbf{v}$. The partial derivative of $t(\theta)$ w.r.t $\theta_i$, is then:

$$\frac{\partial t}{\partial \theta_i} = \sum\limits_{e \in OutEdges(\textbf{w})} \delta_{e} $$ 

 Where $\textbf{w}$ is the corresponding $\theta$\textit{-node} for $\theta_i$. Backpropagation thus can be used to compute the gradient for the loss of a given training sample in a feed-forward network, provided that the activation functions and loss function are easy to differentiate\footnote{This also applies for convex functions which are non-differentiable (e.g $ReLU$), in which case a subgradient is used.}. The process of computing the gradient on a 3-layered network is described in figure \ref{fig:backprop}.

\begin{figure}
    \centering
    \includegraphics{Figures/Electron.pdf}
    \caption{Backpropagation demonstrated on a 3-layered feed forward neural network}
    \label{fig:backprop}
\end{figure}

\subsection{Model Regularization and Overfitting}

As with any machine learning model, neural networks are also capable of overfitting the training data. This is especially true for deep networks, which have a very large amount of parameters and therefore more complexity. Simply picking the set of parameters that minimizes the loss on the training set is likely to overfit, at least to some degree. The common method of applying a regularization term (e.g $l_1$, $l_2$ regularization) in addition to the loss, can also be applied to the training process of a neural network. First, we define the \emph{cost} of a parameter vector $\theta$ w.r.t a of training sample $(\vec{x_i}, \vec{y_i})$ as the sum of its loss and a regularization term $R(\theta)$:

$$ Cost(\theta, \vec{x_i}, \vec{y_i}) = \mathcal{L}(\textbf{f}(\vec{x_i}, \theta), \vec{y_i}) + \lambda \cdot R(\theta) $$

Where $\lambda$ is a hyperparameter. Then, we modify the training process by minimizing the average cost instead of the average loss, so the function $t$ subject to minimization becomes:

$$ t(\theta) = \frac{1}{|Train|}\sum_{(\vec{x_i}, \vec{y_i}) \in Train} Cost(\theta, \vec{x_i}, \vec{y_i}) $$

Assuming that $R$ can be differentiated efficiently, the training process can just as well be applied to the modified $t$.

\subsubsection{Validation Loss and Early Stopping}

A common machine learning methodology for detection and prevention of overfitting is the use of a validation set. A validation set is a set of samples held out of the training set, on which the performance of a trained model is measured. As the validation samples had not been used for training, a high performance on the validation set is an indication of the model being able to generalize well. On the other hand, a model with a high performance on the training set and a low performance on the validation set, had likely to overfit. 

A validation set can be further utilized for prevention of overfitting, by examinig the loss on the validation set (the validation loss) throughout training. As training progresses, the loss on the training set generally decreases until reaching a local minima. The validation loss is also expected to decrease, until the model had overfitted the training set. At this point, the training loss may keep decreasing, but the validation loss may increase due to overfitting. A detection of such increase in validation loss may imply that the model had overfitted, and further training could hurt performance. Training is stopped at this point, and the set of parameters with the lowest validation loss seen throughout training is chosen. This adjustment to the training process is known as Early-Stopping. 

Detecting the point where the validation loss begins to increase due to overfitting is not so trivial. While the training loss is expected to decrease in general, specific training steps may increase it due to the fact the (a) the gradient of the loss is only estimated, and (b) the learning rate may be too big. Similarly, the validation loss may increase in some training steps before decreasing into its optimal value. Therefore, a single increase in validation loss is not enough. Some variations of early stopping does not allow stopping before a certain amount of epochs is completed. Others will stop only when the validation loss had not been decreased below its minimum in the last window of iterations. Another variation allows the training process to continue until the training loss stabilizes, and only then the best parameters are chosen. This variation have a low risk of stopping too early, at the cost of a longer training process.

\subsubsection{Dropout Regularization}

Dropout (todo:ref) is another technique for overfit prevention and regularization of a neural network. At each training step, the network structure is slightly altered by randomly removing hidden neurons from the network. Each neuron is removed independently with a probability of $p$ (a hyperparameter). The output of each neuron subject to dropout is rescaled by $1/p$ to preserve the average neuron output\footnote{This variation of dropout is known as Inverted Dropout}. Both dropping and rescaling of neurons take place only during training. 

As the dropout technique involves many model variations, it can be considered as an approximation of model ensembling, which is known to reduce generalization errors. Dropout has been consistently shown to be effective across many different tasks, often replacing more conventional methods such as the aforementioned $l_1,l_2$ regularization.

% \begin{tikzpicture}
% \begin{axis}[
%     axis lines = left,
%     xlabel = $x$,
%     ylabel = {$f(x)$},
% ]
% %Below the red parabola is defined
% \addplot [
%     domain=-10:10, 
%     samples=100, 
%     color=red,
% ]
% {x^2 - 2*x - 1};
% \addlegendentry{$x^2 - 2x - 1$}
% %Here the blue parabloa is defined
% \addplot [
%     domain=-10:10, 
%     samples=100, 
%     color=blue,
%     ]
%     {x^2 + 2*x + 1};
% \addlegendentry{$x^2 + 2x + 1$}
 
% \end{axis}
% \end{tikzpicture}

\subsection{Recurrent Neural Networks}

\begin{figure}
    \centering
    \includegraphics{Figures/Electron.pdf}
    \caption{A Recurrent neural network operating on a sequence $x_1,..,x_k$. An RNN function transforms an input $x_t$ and the current state $h_t$ into an output $y_t$ and the next state the $h_{t+1}$}
    \label{fig:rnn}
\end{figure}

A Recurrent Neural Network (RNN) is one class of neural network models aiming to deal with sequence processing. In contrast to feed-forward networks, where the input and output dimensions are fixed, RNNs are able to process and produce sequences of varying lengths. RNNs have proven to be extremely successful in a wide variety of NLP tasks, which mostly deal with processing and producing word sequences. 

A typical RNN is stateful - it processes one sequence item at a time, while maintaining an internal state. For each sequence item $x_t$ (and provided the current internal state $h_t$), the RNN produces an output $y_t$ and an updated state $h_{t+1}$ to be used when processing the next item (see figure \ref{fig:rnn}. An RNN models the above computation as a single function, applied repeatedly over a sequence $x_1, ... ,x_k$:

$$ RNN(x_t, h_t) \rightarrow y_t, h_{t+1} $$

The initial state $h_1$ is usually defined to be a zero vector, a randomly initialized vector, or a trainable model parameter. 

An RNN function could be as simple as a single layered feed-forward network. The following is an example\footnote{This is in fact the SimpleRNN module from the DyNet framework (todo ref)} of such RNN, where the output is also used as the next hidden state:

$$h_{t+1} = tanh(W_x \cdot x_t + W_h \cdot h_t + b)$$
$$y_t = h_{t+1}$$

The training process of an RNN is similar to that of a feed forward network: minimizing a loss function using (a variant of) SGD. Computing the gradient of the loss is usually done using \textbf{Backpropagation-Through-Time}, which briefly consists of performing backpropagation on the unfolded computation graph, treating each RNN unit as a different component, and then taking the sum of the gradients of the shared parameters. 

A common pattern is to have an RNN as a part of a larger model, where the outputs of the RNN are fed into a subsequent network for further processing. For instance, consider the task of Part-of-Speech (POS) tagging, where each word in a sentence of length $k$ (represented by a sequence of $k$ vectors $w_1,...,w_k$\footnote{Word vector representations are discussed in sections \ref{section:lookuptables} and \ref{section:wordembeddings}}), is to be assigned with its corresponding POS label. A typical RNN based model could consist of feeding the word vectors through an RNN, yielding $k$ outputs $y_1,...y_k$. Each RNN output could later be fed into a feed-forward network with a Softmax output layer, computing POS label probabilities for each word. While it would be possible to bypass the RNN and feed the word vectors to the FFN directly, the RNN outputs have the advantage of being \emph{contextualized}: an RNN output for a given word is also a function of the previous words, which the network is expected to compute in a manner useful for the task at hand. An output $y_i$ can thus be considered as a \emph{contextualized domain-specific representation} of the $i$-th word in the sentence.

\subsubsection{Bidirectional RNNs}

A contextualized representation generated using a single RNN has the limitation of being dependent solely on the inputs preceding the corresponding sequence item. This prevents the representation from incorporating important information signaled by following items. For instance, the words following a preposition in a sentence are sometimes crucial for determining the role of the preposition. A frequently used method for overcoming this is to introduce a second RNN unit (with the same architecture, but different set of parameters) and apply in on the sequence in the opposite direction. The contextualized representation is then the concatenation of both outputs:

$$ RNN_{\rightarrow}(x_t, h^{\rightarrow}_{t}) = y^{\rightarrow}_t,h^{\rightarrow}_{t + 1} $$
$$ RNN_{\leftarrow}(x_t, h^{\leftarrow}_{t}) = y^{\leftarrow}_t,h^{\leftarrow}_{t - 1} $$
$$ y_t = [y^{\rightarrow}_t:y^{\leftarrow}_t] $$

The resulting representation then encapsulates context from both sides of the represented sequence item.

\subsubsection{Stacking RNNs}

The idea of stacking RNNs together follows the overall success of deep networks in learning complex features. A stacked RNN Architecture consists of applying multiple RNN units over a sequence, forming multiple layers of produced sequences. Each layer processes and recombines information from previous layers, creating representations that perhaps encode features with higher level of abstraction. A network consisting of $L$ stacked RNN layers is defined as follows:

$$ RNN^1(x_t, h^1_t) = y^1_t, h^1_{t+1} $$
$$ RNN^2(y^1_t, h^2_t) = y^2_t, h^2_{t+1} $$
$$ ... $$
$$ RNN^k(y^{k-1}_t, h^k_t) = y^k_t, h^k_{t+1} $$
$$ ... $$
$$ RNN^L(y^{L-1}_t, h^L_t) = y^L_t, h^L_{t+1} $$
$$ y_t = y^L_t $$

The set of RNN units $RNN_1,..,RNN_L$ share the same network architecture, with each unit having its own set of trainable network parameters. In the bidirectional setting, RNN stacking is applied similarly in both directions.

\subsubsection{Long-Short-Term-Memory (LSTM)}

\begin{figure}
    \centering
    \includegraphics{Figures/Electron.pdf}
    \caption{An LSTM network}
    \label{fig:lstm}
\end{figure}

Vanilla RNNs are limited in their ability to capture long range dependencies. For instance, consider the task of assigning the syntactic heads of sentence words. An RNN learning this task could be significantly affected by this limitation, as the distance between a word and its head might be long, causing the network to "forget" the existence of the head by the time the word is reached. \textbf{Long-Short-Term-Memory (LSTM) networks} (todo: ref) is an RNN implementation designed to overcome this limitation. It was shown to be extremely successful at a variety of tasks, which made it tremendously popular and one of the most widely used RNN architectures across the literature.  

LSTM is an implementation of the RNN interface described above. Its internal state is split into two vectors: the cell state $c_t$ and the hidden state $h_t$. The LSTM architecture is comprised of the following equations, defining the current output $y_t$ and next cell state $c_{t + 1}, h_{t + 1}$ as a function of the current cell state $c_t, h_t$ and input sequence item $x_t$:

\begin{equation}
   f_t = sigmoid(W_f[x_t;h_t] + b_f) 
   \label{eq:lstm_ft}
\end{equation}
\begin{equation}
   i_t = sigmoid(W_i[x_t;h_t] + b_i)
   \label{eq:lstm_it}
\end{equation}
\begin{equation}
   \hat{C}_t = tanh(W_C[x_t;h_t] + b_C)
   \label{eq:lstm_cht}
\end{equation}
\begin{equation}
   C_{t + 1} = f_t \circ C_t + i_t \circ \hat{C}
   \label{eq:lstm_ct+1}
\end{equation}
\begin{equation}
   o_t = sigmoid(W_o[x_t;h_t] + b_o)
   \label{eq:lstm_ot}
\end{equation}
\begin{equation}
   y_t = o_t \circ tanh(C_{t + 1})
   \label{eq:lstm_yt}
\end{equation}
\begin{equation}
   h_{t + 1} = h_{t + 1}
   \label{eq:lstm_ht+1}
\end{equation}

The LSTM archirtecture is depicted in figure \ref{fig:lstm}. The cell state is updated in two stages: forgetting old information, and inserting new information. $h_t$ and $x_t$ are fed into a sigmoid layer, computing the \emph{forget vector} $f_i$ (sharing the same dimension of the cell state) with all elements ranging from 0 to 1 (eq. \ref{eq:lstm_ft}). Each element represents the portion of the corresponding element in the cell state to forget. Forgetting is then performed by an element-wise multiplication of the current cell state $C_t$ and $f_t$ (first half of eq. \ref{eq:lstm_ct+1}). This updating technique is known as \emph{gating}. 
Then, the network computes the new information to be added to the cell state $\hat{C}_t$ by feeding $h_t$ and $x_t$ into a $tanh$ layer (eq. \ref{eq:lstm_cht}). They are also used to compute a \emph{insert vector} $i_t$ similar to $f_i$, determining the portion of the new information to be added into the cell state (eq. \ref{eq:lstm_it}). The new information vector $\hat{C}_t$ is then multiplied element-wise by $i_t$ and added to the current cell state (second half of eq. \ref{eq:lstm_ct+1}).
Finally, a third sigmoid layer taking $h_t$ and $x_t$ as inputs (producing $o_t$) is used to the determine the portion of the cell state to be used as the LSTM output (eq. \ref{eq:lstm_ot} and \ref{eq:lstm_yt}), which is also used as the next hidden state $h_{t+1}$ (eq. \ref{eq:lstm_ht+1}). 

This architecture explicitly models the notion of state as a memory cell, updated only due to learnable detected triggers. The cell state can be considered as a memory cell, where new information added is to be preserved as long as not explicitly removed. This allows the network to pass detected signals across a long sequence of recurrent iterations.  

\subsection{Lookup Tables} \label{section:lookuptables}

As with most machine learning models, the inputs and outputs of a neural network consist entirely of numeric vectors. Non-numeric input must therefore be vectorized into a numeric representation in a manner suitable for the network. As each neuron typically detects some useful feature based on its inputs, a good vector representation would follow this principle by being comprised of meaningful dimensions, each signaling some independent and useful feature of the represented information.  

Categorical information sources such as words, grammatical dependency labels, and POS labels are frequently used as input features for various NLP tasks. A well known method for representing such features is the one-hot vector representation. Although complete, this representation is very shallow - it does not provide the network with any additional information regarding the relation between the input signal and the particular task at hand. The network is expected to learn how to extract such information by neurons in subsequent layers.  

Another categorical feature representation method, aiming to explicitly model the aforementioned behaviour, is the use of lookup tables. A lookup table allows the network to \emph{learn} vector representations which are useful for the particular task. A categorical feature with a vocabulary of size $|V|$ is represented by a lookup table comprised of $|V|$ vectors of dimension $dim_V$ (a hyperparameter). These vectors are considered as part of the model parameters. They are initialized randomly, and updated during training together with the rest of the parameters.

A lookup table entry is only updated upon training over a sample with the corresponding feature value. A small vocabulary (such as POS labels, usually containing no more then a few dozens of labels) could be well covered by a reasonably sized, task specific training set. The English vocabulary, on the other hand, could require an enormous amount of training samples in order for the network to learn meaningful word representations, even when only considering frequent words. As collecting training samples could be costly for tasks requiring human supervision (which are very common in NLP), such tasks are rarely equipped with the necessary amount of training data. 

A common approach to this problem is to initialize the corresponding lookup table with pretrained word vectors, obtained by a training separate model designed to learn meaningful word vectors in an unsupervised fashion. Such models are described in greater detail in section \ref{section:wordembeddings} (todo the following section). When using pretrained embeddings, it is possible to treat the lookup table as a constant, instead of updating the vectors during training. 

\subsection{Pretrained Word Embeddings} \label{section:wordembeddings}
Word embedding models are a class of learning techniques aiming to map a set of words from a vocabulary into a numeric vector space. A good word embedding model would capture and represent semantic information emitted by the word, typically with the property that words with similar meanings have similar representations. Word embedding models provide a general-purpose set of word vector representations that can later be used by any subsequent task-specific model. They are usually obtained in an unsupervised manner, training a model over a large corpus of unannotated sentences (usually obtained by harvesting various public web resources such as Wikipedia). The development of word embedding models have been a key aspect in the progress of NLP research throughout the last decade.

\subsubsection{Word2Vec}

\begin{figure}
    \centering
    \includegraphics{Figures/Electron.pdf}
    \caption{Word2Vec models: SkipGram (left) and Continuous-Bag-of-Words (right) todo}
    \label{fig:word2vec}
\end{figure}

SkipGram and Continuous-Bag-Of-Words (CBOW) are two similar neural word embedding models jointly referred to as \textbf{Word2Vec} (todo: ref). They have perhaps been the first neural word embedding models to be both rather simple and fast to train, yet very successful at the task. Despite following improvements and progress made in recent years, they are still widely used nowadays due to their simplicity and effectiveness. Many pretrained Word2Vec models are publicly available, eliminating the need to train a model in order to obtain the word vectors. 

Both models follow the \emph{distributional hypothesis}, assuming that distributional properties of a word - specifically, the distribution of context words - correlate with the word and its meaning. They model the relation between the word and its context, by predicting one from the other. The models are deipicted in figure \ref{fig:word2vec}. 

In the \textbf{SkipGram} model, a word vector for a sentence word $w_t$ is used to predict the presence of a context word $w_{t+d}$ $(1 \leq |d| \leq k)$, where a context word is limited to a window of size $k$ (a hyperparameter). The training set therefore consists of word and context word pairs $(w_t, w_{t+d})$. In the simple (yet impractical) formulation of SkipGram, the word vector for $w_t$ is fed into a softmax layer of size $|V|$, producing a probability for each word in the vocabulary $V$ to appear as a context word. The network is then trained to maximize the probability of the correct context word $w_{t+d}$ by minimizing the cross-entropy loss. The word vectors are learned during training (following the lookup table pattern described in section \ref{section:lookuptables}).

\textbf{Continuous-Bag-Of-Words (CBOW)} goes the other way around, predicting the word $w_t$ is by its context $\{w_{t+i} | 1 \leq |d| \leq k\}$. The average of all context word vectors are fed into a softmax layer assigning each vocabulary word with a probability, similarly to the SkipGram model. 

In practice, computing a probability for each word in the vocabulary for every training sample is too computationally expensive. Instead, two approximation methods are suggested: Hierarchical Softmax (todo ref) and Negative Sampling. 

\textbf{Hierachical Softmax}, first presented by (todo: ref), is an approximation of full softmax which indirectly models the probability distribution using a binary tree, where leaves represent vocabulary words. Non-leaf nodes are assigned with a branching probability for each child node. This defines a random walk from the root node to a leaf, where the probability to arrive at a certain leaf is the multiplication of the probabilities for each branch in the corresponding path. It follows that computing the probability for a single word requires evaluating about $log_2(|V|)$ probabilities assigned by the model (instead of $|V|$ probabilities in the case of full softmax).

In \textbf{Negative Sampling}, each word is assigned with a \emph{score} rather a probability. The model objective then aims to separate the score of the correct word from the rest, by maximizing the difference between the correct score and the sum of scores assigned to $r$ incorrect words sampled from the vocabulary ($r$ is a hyperparameter). The score of an individual word is modeled as a minus-log-sigmoid layer. Mikolov et al. empirically show that a sample size $r$ as small as $2 - 5$ is suffice for a large training corpus. 

Other than being successful as inputs to subsequent tasks, Word2Vec vectors posses some notable intrinsic properties indicating their success at capturing semantic information. For instance, they are successful at capturing word associations such as $king - male + female \approx queen$, using the cosine similarity measure. This is an indication that the primary goal of constructing word vectors with meaningful \emph{dimensions} is well obtained by Word2Vec models.

\subsubsection{Contextualized Word Embeddings}
Pretrained word embeddings as described above have the inherent limitation of lacking context. For instance, the same word vector representation for "table" is used in "table of contents" and "dinner table" despite the different lexical meaning. A subsequent task-specific model could generate context-aware representations, for instance by feeding the vectors into a recurrent layer such as LSTM. Such tasks, however, usually require some level of human supervision, limiting the size of the obtainable training set and the quality of the learned context-aware representations as a result. 

Instead, a few unsupervised methods for learning context aware word representations have been introduced. A recent one is \textbf{Embeddings from Language Models (ELMo)} (todo ref). The ELMo architecture consists of learning multi-layered LSTMs capable of language modeling. First, a context-insensitive representation is learned for each word in a sentence, by applying a convolutional network over the characters \footnote{Convolutional Networks are a class of neural networks rarely used for NLP, and therefore remain out of scope for this work.}. For each training sentence, these word vectors are fed into a multi-layered forward LSTM followed by a softmax layer, predicting the next word (forward language model). Similarly, the sentence words are fed in reverse order into a backward multi-layered LSTM followed by softmax, predicting the previous word (backward language model). The weights for the backward and forward softmax layers are shared. 

ELMo uses two layers of forward and backward LSTMs. By concatenating the outputs of the forward and backward layers, and considering the bottom layer of context-insensitive vectors, it follows that the model couples each sentence word with three layers of vector representations (where two of them are context aware). Peters et al. (todo ref) demonstrate empirically that syntactic information is better captured at the lower layers of the representation, while the higher levels are better at capturing the semantics of the word.  A subsequent model can use a (learned) linear combination of the three representations, in a manner best suitable for the task at hand. 

The result is an unsupervised, pretrained model capable of producing context-aware word representations for a given sentence. As ELMo receives characters as inputs, it is even capable of producing vectors for words unseen during training. The integration of pretrained ELMo vectors has been consistently shown to boost performance of various NLP models, when compared to previous word embedding methods. 

\subsection{Neural Networks in this work}

In chapter \ref{Chapter2}, a neural classifier capable of PSS disambiguation is presented. It is comprised of a multi-layered bidirectional LSTM followed by two MLP-softmax layers, assigning prediction probabilities for the Role and Function PSS of a preposition. Words are embedded using SkipGram or ELMo, and various syntactic information is also embedded as input to the network with the use of lookup tables. 

Chapter \ref{Chapter3} deals with incorporating PSS features into an existing neural Prepositional-Phrase-Attachment\footnote{The Prepositional-Phrase-Attachment is thoroughly described in section \ref{sec:ppatt}} model in an attempt to boost attachment accuracy. The original model, by Belinkov et al. 2014 (todo ref), consists of a two-layerd feed forward network assigning scores to attachment candidates, and trained using the hinge-loss. They use \emph{syntactic} word embeddings, obtained using a slight modification of SkipGram which uses syntactic word dependencies to define the context of a word, following Bansal et al. (2014) (todo ref). The PSS information is integrated into the model using lookup tables.

(todo chapter 4)

\pagebreak

%-----------------------------------
%	SECTION 3
%-----------------------------------
\section{Natural Language Parsing and the Prepositional Phrase Attachment Problem} \label{sec:ppatt}

Chapter \ref{Chapter2} examines how beneficial the information provided by PSS could be for Prepositional-Phrase Attachment Disambiguation (PPA)  - one of the highly known kinds of syntactic ambiguity in sentence processing literature. In this section we describe this problem in detail, and explain the motivation behind using PSS information as a mean to improve disambiguation accuracy. 

\subsection{Sentence Parsing}

The PP-Attachment problem can be considered as a subtask of sentence parsing - a key task in NLP that is used heavily as as a preprocessing step for various kinds of NLP applications.  Sentence parsing is the process of analyzing a given natural language sentence, and as a result, generating a tree-like structure called a parse-tree that references the original sentence, and enriches it with additional layers of information through structure and the use of labeled arcs and/or nodes. The task of sentence parsing can be thought of as picking the correct parse-tree out of a set of valid candidates for a given sentence. Most modern parsers are trained to do so using machine learning techniques and existing manually-annotated corpora.

 Different parsing tasks typically vary from one another by the structure of the resulting parse-tree (dependency based/constituency based), and the kind of information represented (on the spectrum between mostly syntactic to mostly semantic).

\subsubsection{Dependency vs. Constituency Based Parsing}

In \emph{dependency parsing} tasks, the nodes in the tree usually represent words in a sentence, and tree arcs connect a dependent word (and its sub-tree) with its head (or governor), sometimes with a complementary dependency label, indicating the kind of dependency relations between different parts of the sentence. A notable and widely used example of such parsing task is Universal Dependencies (UD) (todo:ref), which is also extensively used in this work. The UD project aims to facilitate a dependency annotation scheme that can be applicable to a variety of languages, along with a large corpora of annotated sentences to support it. The basic UD dependency representation (see figure todo) forms a dependency tree as described above. The UD schema is explained in greater detail in section (todo ref).

Constituency based parsing tasks assign labels to words and phrases in a sentence. Depending on the task and the corresponding parse-tree schema, the parse-tree may or may not cover the entire sentence, contain multiple layers of labels, and in the latter case - it may or may not be projective (meaning that a labeled phrase cannot only be partially contained in another). Projective schemes typically form a tree similar to that of a context free grammar (and in fact, many constituency based parsing tasks are based on such grammar). The Penn Treebank - a large corpus of annotated sentences from the World Street Journal, uses a prominent projective constituency-based annotation scheme (see figure todo). It is wildey used in the field of NLP in general, and as a training data for PP-Attachment research in particular.

\subsubsection{Syntactic vs. Semantic Parsers}

A key distinguisher between different parsers is the kind of information that they capture, and whether it has to do with syntax (i.e part-of-speech, low-level dependencies such as: "A is a modifier of B", "C is a determiner of D"), or with semantics (such as: “A is a SCENE that B is a PARTICIPANT of, and C is an ACTION performed by B on OBJECT D”, where A,B,C,D are sentence words or phrases and SCENE, PARTICIPANT, ACTION, OBJECT are from a bank of labels). The distinction between semantic and syntactic information is not well defined, and most parsing schemes are in fact somewhere in the middle of this spectrum. While UD and Penn-Treebank are considered to be mostly syntactic, Abstract Meaning Representation (AMR) and Universal Conceptual Cognitive Annotation (UCCA) are two recent parsing schemas that are closer to the semantic edge of the spectrum. As demonstrated in figure todo, they both add a layer of semantic information in the form of a directed-acyclic-graph (todo: verify for AMR). 

\subsection{The PP-Attachment problem}

The PP-Attachment problem involves deciding which part of a sentence  - typically a noun-phrase (NP) or a verb-phrase (VP) - is modified by a prepositional phrase (PP). That part of the sentence (or its head word) is considered to be the governor or head of the PP. In most cases, a preposition is followed by a NP, which is considered to be the object of the preposition. Though it may seem mostly syntax related, the decision process significantly involves the semantics of the sentence. For instance, consider the following sentences and their corresponding parse trees: 

\begin{enumerate}
    \item[1.] [I]$_{NP}$ [am reading]$_{VP}$ [a book]$_{NP}$ [with [a hard cover]$_{NP}$]$_{PP}$. 

    (todo: two parse trees, both valid but only the NP attached one is correct)

[with a hard cover]$_{PP}$ modifies [the book]$_{NP}$ (the book has a hard cover), hence (b) is the parse tree with the correct attachment. “book” is the governor of “with” and “cover” is the object.

    \item[2.] [I]$_{NP}$ [am reading]$_{VP}$ [a book]$_{NP}$ [with  [my children]$_{NP}$]$_{PP}$. 

    (todo: two parse trees, both valid but only the VP attached one is correct)

[with my children]$_{PP}$ modifies [am reading]$_{VP}$ (I am reading with my children), hence (a) is the parse tree with the correct attachment. “reading” is the governor of “with” and “children” is the object.

    \item[3.] [I]$_{NP}$ [am reading]$_{VP}$ [a book]$_{NP}$ [about [psychology]$_{NP}$]$_{PP}$. 

    (todo: two parse trees, both valid and both may be correct)

In this example, both attachments are semantically correct: [about psychology]$_{PP}$ could be attached to [a book]$_{NP}$ (The book was about psychology), but it could also be attached to [am reading]$_{VP}$ (I am reading about psychology).

    \item[4.] [I]$_{NP}$ [was watching]$_{VP}$ [the movie]$_{NP}$ [with my friends]$_{PP}$. 

    (todo: two parse trees, both valid but only one is correct, we don’t know which one but one is more likely) 

    The parse trees reflect two valid but different interpretations to this sentence: in one, [with my friends]$_{PP}$ is attached to [was watching]$_{VP}$ (the speaker was with her friends while watching the movie), and in the other, it is attached to [the movie]$_{NP}$ (the speaker’s friends appeared in the movie).

\end{enumerate}
 

All parse trees presented here are syntactically valid, thus deciding which is correct requires a deeper understanding of the semantics conveyed by the sentence and its context.
Sentences (1) and (2) differ only in the object of the preposition (a hard cover/my children), and it’s our knowledge of the real world which dictates that “a hard cover” is a property of a book and that “my children” are participants in the reading activity. Sentence (3) shows a case where there is more than one correct attachment - demonstrating that PP-Attachment is not always binary that sense. Sentence (4) is an even harder example - it has two semantically-valid interpretations, which requires a context broader than sentence level for disambiguation.  
\subsection{PP Attachment Disambiguation Approaches}

\subsubsection{As a sub-task of sentence parsing}
As demonstrated in the example above (todo: ref), the choice of attachment has a direct effect on the structure of a corresponding prediction of a syntactic parse tree. Furthermore, considering that different attachments correspond to different sentence meanings, semantic parsers are also to be affected by this choice. 
(todo - show a ucca and ud parsing mistakes that arise from pp-att)
This makes PP-Attachment a sub-task of many parsing tasks, though many parsers do not address it explicitly. Transition based parsing (todo: ref) for example, is a parsing architecture that generates the output parse-tree incrementally by applying a set of transitions - each can modify the tree, or the internal state of the parser. The next transition to perform is chosen depending on the current internal state, the input, and a set of (learned, model dependent) parameters. Hence, the architecture of such parsers does not deal explicitly with PP-Attachment, or any syntax ambiguity, but instead they rely on the underlying decision-making model to capture implicit, learned representations of such ambiguities through the process of training over human annotated corpora. 


A comparative evaluation of parser performance on the Wall Street Journal corpus by Kummerfeld et al. (2012) indicates that PP attachment is the largest source of errors across all parsers.  Previous work had been made on the subject of improving PP-Attachment accuracy in the context of full parsing. (Belinkov et al, 2014) shows that incorporating PP-Attachment specific features into an existing parser can increase parsing accuracy. They trained a PP-Attachment disambiguation model, and incorporated its predictions as features into the RBG dependency parser (todo: ref), increasing PP-Attachment accuracy from 88.4\% to 90.1\%.  (Agirre et al.,2008) incorporated semantic features based on WordNet (todo: ref and describe) into two dependency parsers: the Bikel parser (todo: ref) and the Charniak parser (todo: ref). They report an increase in overall parsing accuracy and PP-Attachment accuracy in particular. [Henestroza & Candito 2011] took the parser-agnostic approach of post-parsing error correction - they trained a feature-rich model which modifies an existing parse-tree by re-attaching some of its nodes. 

\subsubsection{As an isolated task}

Despite all said above, the majority of the work done on the subject is in the context of PP-Attachment alone. A typical attempt at solving PP-Attachment assumes the following problem formulation: given a verb (V), a noun (N), a preposition (P), and an optional second noun (N2), should P be attached to V or N? In sentence (1), for example, the corresponding assignments are: (V=reading, N=book, P=with, N2=cover), and N=book is the correct attachment.  As pointed out by (Atterer & Schütze 2007), this approach is problematic as it assumes an attachment-candidates oracle that narrows down the list of candidates to a single noun and verb. In practice, such oracle is not available for parsers, which may deal with long and complex sentences, containing much more than two candidates for a given preposition. For this reason, accuracies reported for such scenarios are to be taken with a grain of salt as to whether they would replicate in a practical NLP application. (Atterer & Schütze 2007) argue that since oracle-based approaches tend to perform better, PP attachment should not be evaluated in isolation, but instead as an integral component of a parsing system, without using information from the gold-standard oracle. (Belinkov 2014) partially addresses this problem by presenting a PP-Attachment disambiguation model that can take an arbitrary number of candidates, and a dataset that contains multiple candidates per preposition, and is only loosely based on the gold parse-tree (see section todo for a detailed description).    

Unsupervised PP-Attachment methods are mostly based on co-occurrence frequencies of a preposition and a head candidate, indicating which is more likely to be correct (Hindle, Rooth 1993).  (Ratnaparkhi, 1998) takes a similar approach of learning from heuristically identified unambiguous PP attachment cases.  (Medimi, Bhattacharyya 2007)  additionaly incorporate WordNet senses to handle data sparsity. The use of WordNet  information is common in the supervised setting as sell - (Stetina, Nagao 1997) for example, train a PP-Attachment decision tree over word sense information of the candidates. (Kawahara, Kurohashi 2005), (Olteanu & Moldovan 2005) incorporate co-occurance frequencies extracted from the web into a Support-Vector-Machine learning model, trained on a manually annotated PP attachment corpus. The neural models from (Belinkov, 2014) make use of WordNet and VerbNet features, but they mostly rely on pretrained, dependency based word embeddings. (Dasigi et al 2017) is an expansion of (Belinkov, 2014), which uses a contextualized, learned, WordNet based sense word embeddings instead of type level embedding, using Bi-LSTM cells.


\subsection{PP Attachment in this work}

Chapter 2 (todo ref) examines how beneficial PSS information could be to an existing PP-Attachment model. As shown in example (todo: ref), different PP-Attachment candidates represent different semantic interpretations of a sentence. In such cases, each interpretation may yield a different PSS for the corresponding preposition. For instance, consider sentences (1) \emph{"I am reading a book \textbf{with} a hard cover"} and (2) \emph{"I am reading a book \textbf{with} my children"} - in (1), \emph{“with”} would be annotated with (todo CHARACTERISTIC??) while in (2) the correct annotation would be (todo PARTICIPANT??). This provides evidence to believe that a PP-Attachment disambiguator might benefit from using PSS information. We experiment with an existing PP-Attachment model from (Belinkov et al 2014 todo), by incorporating PSS features onto it, expecting to see a boost in disambiguation accuracy.

\pagebreak

%-----------------------------------
%	SECTION 5
%-----------------------------------
\section{NLP Specifications Used In This Work}

Some commonly used NLP specifications (and corresponding parsers or data sources) have been used or mentioned throughout this work, mostly in the context of supplying machine learning models with additional layers of information. While not being the prime focus of this work, these specifications are important and sometimes crucial for the success of such models. The sections briefly describe each specification, and its relation to this work. 

\subsection{Universal Dependencies}

\begin{figure}
    \centering
    \includegraphics{Figures/Electron.pdf}
    \caption{An example UD parse tree}
    \label{fig:udparsetree}
\end{figure}

The Universal Dependencies (UD) project (todo ref) aims to facilitate a cross-lingual sentence dependency parsing schema. In the basic form of UD, a sentence is parsed into a dependency-tree structure comprised of words as nodes, with labeled arcs signaling a dependency relation between a child word to its head. The labels indicate the type of relation, which are mostly grammatical (e.g. \emph{det} - child is the determiner of the head, \emph{nsubj} - the (nominal) parent is the subject of the clause), but sometimes bear additional low-level information that could be considered semantic (e.g. \emph{ccopm} - the child clause represents a core argument of the parent verb). See figure \ref{fig:udparsetree} for an example of an english sentence with its corresponding UD parse tree.

The UD project is an evolution of Stanford Dependencies (todo ref (de Marneffe et al., 2006, 2008, 2014)), Google universal part-of-speech tags (todo ref Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (todo Zeman, 2008). It is continuously developed and refined, with a growing amount of manually annotated corpora and parsers available for a variety of languages. 

The information obtained by a UD parse-tree is used in this work for PSS disambiguation (\ref{Chapter2}). Arising from the observation that the governor and object of a preposition are in many cases crucial for deciding the true semantic relation (todo ref to background), the governor and object are incorporated to the model as additional input features. The UD parse-tree is used to determine the governor and object as a preprocessing step (described in todo ref). Depending on the specific setting, the parse trees are either obtained using the Stanford CoreNLP parser (todo ref), or using human annotation (which is available in the STREUSLE PSS corpus). In chapter \ref{Chapter3}, the aformentioned PSS model is applied on a PP-Attachment dataset, similarly applying this preprocessing step.

\subsection{Named Entity Recognition}
\textbf{Named Entity Recognition (NER)} is the task of automatically identifying and categorizing mentions of named entities in a given unstructured text. Such entities include locations, organizations, person names, temporal expressions (such as dates, day of week, month, etc.), numerical quantities, and so on.

The NER parser of Stanford CoreNLP is used in this work to enhance the input representation of the PSS classifier described in chapter \ref{Chapter2}. The out-of-the-box CoreNLP NER parser uses a combination of rule-based and statistical NER models, supporting a total of 12 labels covering named, numerical, and temporal types of entity categories. An example of a sentence with its NER labels, as parsed by CoreNLP, is \emphh{"[Jess Bezos]$_{PERSON}$ is the [CEO]$_{TITLE}$ of [Amazon]$_{ORGANIZATION}$ since [July 5, 1994]$_{DATE}$"}. As demonstrated in this example, a NER label of a prepositional phrase's object ([Amazon]$_{ORGANIZATION}$) may provide a great deal of information regarding the relation conveyed by the preposition (corresponding in this example to the ORG\_ROLE PSS label for \emph{of}). This is the motivation behind incorporating NER labels as a feature onto the PSS classifier.

\subsection{WordNet and VerbNet}

WordNet (todo ref) is a lexical database for the English language. It was created in the Cognitive Science Laboratory of Princeton University in 1985, and has been continuously developed and refined since. The WordNet project aims to cover the entire English vocabulary, with the exception of function words.  Words that are roughly synonymous are mapped into distinctive sets (called \emph{synsets}). Synsets are then linked together by means of semantic relations, forming a comprehensive knowledge base of the different lexical meanings of words, and their relation to other words. Though can be used directly by human users as a form of dictionary or thesaurus, its primary goal is to assist NLP applications with external lexical knowledge. 

A commonly used relation is the \emph{hypernym} - indicating that one word is a generalization of the other (\emph{mammal} is a hypernym of \emph{dolphin}). Two words sharing a hypernym are thus likely to be similar, at least in some aspects. The set of hypernyms of a word, along with other WordNet relations, is used throughout the literature to define word similarity measures, construct and enrich word vectors and thus allow models to better handle out-of-vocabulary words, or a lack of sufficient training data. The use of WordNet in this manner was more common in the pre neural-word embeddings era, which also capture these properties, are more comprehensive, and are a better fit for neural models.

VerbNet (todo ref Kipper-Schuler 2006) is another lexical database focused on English verbs. Similar verbs are grouped together into verb classes (similarly to synsets). Each verb class is described by a set  of frames, demonstrating the different uses of the verb in a sentence by providing examples, and a set of structured syntactic and semantic settings. 

In the context of this work, WordNet and VerbNet are used in a variety of PP Attachment models throughout the literature, and specifically by the neural model of (Belinkov et al 2014 todo ref) to enrich the feature vectors of the preposition and head candidates. This model is used in Chapter \ref{Chapter3}, and described in detail in section (todo ref).

\pagebreak