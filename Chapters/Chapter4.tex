% Chapter Template

\chapter{Preposition Supersense Disamb×Ÿguation in a Multilingual Setting } % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Overview}

In this chapter, we examine the transferability of the knowledge acquired by a PSS disambiguator between different languages and domains. Specifically, we evaluate the PSS model presented in chapter \ref{Chapter2} - trained solely using the STREUSLE corpus of English web reviews - on the recently introduced Mandarine Chinese PSS corpus by (\cite{chinesecorpus}). The Chinese corpus consists of the first 20 chapters of \textit{The Little Prince}, along with manually annotated PSS labels for all identified adpositions. First, words in the Chinese corpus are automatically aligned to the corresponding words in the parallel English translation (section \ref{sec:chinesecorpus}). As the word alignment is noisy and incomplete, the PSS model and training process are slightly altered to better handle the information gaps (section \ref{sec:pssmodelmod}). The PSS disambiguation accuracy on the Chinese corpus is reported in section \ref{sec:zhpssres}.

(todo more about results)

\section{Aligning the Parallel Corpus} \label{sec:chinesecorpus}

The Chinese corpus was word-aligned to the corresponding English translation using TsinghuaAligner (\cite{liu2014contrastive}) - an unsupervised log-linear model trained using a \textit{Contrastive Learning} objective: assigning higher probabilities to correct sentence pairs, than to noisy ones generated by shuffling pairs in the training set. 

The alignment process is rather noisy and incomplete - out of 518 annotated Chinese prepositions, only 98 (19\%) were aligned to English prepositions. 49 (9\%) Chinese prepositions were aligned to non-prepositional English words, and the remaining 371 (72\%) prepositions did not receive an alignment at all. The PSS disambiguator also considers the governor and object of each preposition, but since these are not available for Chinese, it was modified to use the parent and grandparent of the preposition according to the UD tree, which provide a close approximation. The parent and grandparent were determined according to the English UD tree in the case of aligned prepositions, and the Chinese UD tree in the case of unaligned prepositions. The UD trees were obtained using the CoreNLP parsers for Chinese and English (\cite{manning14stanford}). 

The parents and grandparents of an unaligned preposition, were also not fully aligned: for the 371 unaligned Chinese prepositions, 138 (37\%) of the parents and 162 (43\%) of the grandparents remained unaligned. The remaining auxiliary information required by the PSS model (UD labels, POS, NER, etc.) was obtained by applying the CoreNLP parser on the English corpus. 

\section{PSS Model and Training Adjustments} \label{sec:pssmodelmod}

The PSS model presented in Chapter \ref{Chapter2} was slightly altered to better handle the noisy alignment of words between the Chinese and the English corpora. As previously mentioned, the model uses the parent and grandparent of a preposition instead of the governor and object. For the model to handle unaligned prepositions, prepositions are randomly replaced with "blank" tokens during training, in a tunable probability. "blank" tokens are not fed into the LSTM layer, and thus do not have a contextualized representation. In such cases, a single (trainable) parameter vector is used as the contextualized representation portion of the enriched preposition vector (section \ref{sec:enrichedvec}). Similarly, the parent and grandparent contextualized representations portion of the enriched preposition vector are randomly "dropped" by replacing with a single parameter vector, representing a missing parent/grandparent. 

\section{Results} 

The modified PSS model was trained as described in Chapter \ref{Chapter2}, using the STREUSLE training set for training, and the development set for hyperparameter tuning. The model, trained solely on English sentences, is then evaluated on the Chinese corpus with the use of the word-aligend, parallel English translation. The disambiguation accuracy of the model, as well as the MF-Prep baseline (section \ref{sec:pssbaselines}), are reported in table \ref{tab:chineseresults}.

\begin{table}[]
    % \setlength{\tabcolsep}{6pt} % Default value: 6pt
    \newcommand\Tstrut{\rule{0pt}{2.6ex}}       % "top" strut
    \newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}} % "bottom" strut
    \newcommand{\TBstrut}{\Tstrut\Bstrut} % top&bottom struts
    
    \renewcommand{\arraystretch}{1}
    \newcommand{\score}[2]{#1 {\footnotesize ($\pm$#2)}}
    \newcolumntype{C}{c}
    \centering
    \begin{tabular}{*{9}{C}}
         \toprule
         System    & \multicolumn{2}{C}{STREUSLE acc.} & \multicolumn{6}{C}{Chinese acc.}  \\ \cmidrule(lr){4-9}
                   &    \multicolumn{2}{C}{} & \multicolumn{2}{C}{All (?)}      & \multicolumn{2}{C}{Aligned (?)} & \multicolumn{2}{C}{Aligned To Prep. (?)} \\ 
         \cmidrule(lr){2-3}
         \cmidrule(lr){4-5}
         \cmidrule(lr){6-7}
         \cmidrule(lr){8-9}
                                    & Role & Fxn.     & Role  & Fxn.   & Role  & Fxn.  & Role  &  Fxn. \Bstrut \\
         \hline
         
         MF-Prep & 12.34 & 12.34 & 12.34 & 12.34 & 12.34 & 12.34 & 12.34 & 12.34 \Tstrut \\
         Neural  & 12.34 & 12.34 & 12.34 & 12.34 & 12.34 & 12.34 & 12.34 & 12.34 \\
         
         \bottomrule
    \end{tabular}
    \caption{PP-Attachment disambiguation accuracy of the best performing model in each setting (average and standard deviation over 5 different training instances).}
    \label{tab:chineseresults}
\end{table}






