% Chapter Template

\chapter{Preposition Supersense Disamb×Ÿguation in a Multilingual Setting } % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Overview}

In this chapter, we examine the transferability of the knowledge acquired by a PSS disambiguator between different languages and domains. Specifically, we evaluate the PSS model presented in chapter \ref{Chapter2} - trained solely using the STREUSLE corpus of English web reviews - on the recently introduced Mandarine Chinese PSS corpus by \cite{chinesecorpus}. The Chinese corpus consists of the first 20 chapters of \textit{The Little Prince}, along with manually annotated PSS labels for all identified adpositions. First, words in the Chinese corpus are automatically aligned to the corresponding words in the parallel English translation (section \ref{sec:chinesecorpus}). As the word alignment is noisy and incomplete, the PSS model and training process are slightly altered to better handle the information gaps (section \ref{sec:pssmodelmod}). The PSS disambiguation accuracy on the Chinese corpus is reported in section \ref{sec:zhpssres}. The near-baseline levels of disambiguation accuracy on the Chinese corpus confirm the significance of domain and language for model performance.

\section{Aligning the Parallel Corpus} \label{sec:chinesecorpus}

The Chinese corpus was word-aligned to the corresponding English translation using TsinghuaAligner (\cite{liu2014contrastive}) - an unsupervised log-linear model trained using a \textit{Contrastive Learning} objective: assigning higher probabilities to correct sentence pairs, than to noisy ones generated by shuffling pairs in the training set. 

The alignment process is rather noisy and incomplete - out of 518 annotated Chinese prepositions, only 98 (19\%) were aligned to English prepositions. 49 (9\%) Chinese prepositions were aligned to non-prepositional English words, and the remaining 371 (72\%) prepositions did not receive an alignment at all. The PSS disambiguator also considers the governor and object of each preposition, but since these are not available for Chinese, it was modified to use the parent and grandparent of the preposition according to the UD tree, which provide a close approximation. The parent and grandparent were determined according to the English UD tree in the case of aligned prepositions, and the Chinese UD tree in the case of unaligned prepositions. The UD trees were obtained using the CoreNLP parsers for Chinese and English (\cite{manning14stanford}). 

The parents and grandparents of an unaligned preposition, were also not fully aligned: for the 371 unaligned Chinese prepositions, 138 (37\%) of the parents and 162 (43\%) of the grandparents remained unaligned. The remaining auxiliary information required by the PSS model (UD labels, POS, NER, etc.) was obtained by applying the CoreNLP parser on the English corpus. 

\section{PSS Model and Training Adjustments} \label{sec:pssmodelmod}

The PSS model presented in Chapter \ref{Chapter2} was slightly altered to better handle the noisy alignment of words between the Chinese and the English corpora. As previously mentioned, the model uses the parent and grandparent of a preposition instead of the governor and object. For the model to handle unaligned prepositions, they are randomly replaced with "blank" tokens during training, in a tunable probability. "blank" tokens are not fed into the LSTM layer, and thus do not have a contextualized representation. In such cases, a single (trainable) parameter vector is used as the contextualized representation portion of the enriched preposition vector (section \ref{sec:enrichedvec}). Similarly, the parent and grandparent contextualized representations portion of the enriched preposition vector are randomly dropped, by replacing them with a single parameter vector, representing a missing parent/grandparent. 

\section{Results} \label{sec:zhpssres}

\begin{table}[]
    % \setlength{\tabcolsep}{6pt} % Default value: 6pt
    \newcommand\Tstrut{\rule{0pt}{2.6ex}}       % "top" strut
    \newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}} % "bottom" strut
    \newcommand{\TBstrut}{\Tstrut\Bstrut} % top&bottom struts

    \renewcommand{\arraystretch}{1}
    \newcommand{\score}[2]{#1 {\footnotesize ($\pm$#2)}}
    \newcolumntype{C}{c}
    \centering
    \begin{tabular}{*{9}{C}}
         \toprule
         System    & \multicolumn{2}{C}{STREUSLE acc.} & \multicolumn{6}{C}{Chinese acc.}  \\ \cmidrule(lr){4-9}
                   &    \multicolumn{2}{C}{} & \multicolumn{2}{C}{All (518)}      & \multicolumn{2}{C}{Aligned (147)} & \multicolumn{2}{C}{\makecell{Aligned To\\Prep. (98)}} \\
         \cmidrule(lr){2-3}
         \cmidrule(lr){4-5}
         \cmidrule(lr){6-7}
         \cmidrule(lr){8-9}
                                    & Role & Fxn.     & Role  & Fxn.   & Role  & Fxn.  & Role  &  Fxn. \Bstrut \\
         \hline

         MF-Prep & 40.62 & 53.33 & 36.67 & 41.69 & 48.29 & 49.65 & 68.36 & 69.38  \Tstrut \\
         Neural  & 79.69 & 88.74 & 36.1 & 36.29 & 52.38 & 51.02 & 68.36 & 66.32  \\
         Neural + MF-Prep  & - & - & 37.83 & 42.08 & - & - & - & - \\
         \bottomrule
    \end{tabular}
    \caption{PSS disambiguation accuracy of the MF-Prep baseline, the neural model, and a combined model, on the STREUSLE test set, and the Chinese corpus. For Chinese, we also report the accuracy over the subset of prepositions that received an alignment, and the subset which was aligned to English prepositions.}
    \label{tab:chineseresults}
\end{table}

The modified PSS model was trained as described in Chapter \ref{Chapter2}, using the STREUSLE training set for training, and the development set for hyperparameter tuning. The model, trained solely on English sentences, is then evaluated on the Chinese corpus with the use of the word-aligend, parallel English translation. 

The disambiguation accuracy of the model on English and Chinese are reported in table \ref{tab:chineseresults}. For Chinese, we additionally report the accuracy on two subsets - (i) The subset of Chinese prepositions aligned to any English word, and (ii) The subset which was aligned to English prepositions. We also report the accuracy of the MF-Prep baseline (section \ref{sec:pssbaselines}), which was similarly trained on English. Finally, we also perform an evaluation of a combined model (Neural + MF-Prep). Both models are trained separately, and used in combination for the Chinese evaluation: the neural model for the subset of aligned prepositions, and the MF-Prep baseline model for the remaining unaligned prepositions. 

As expected, the disambiguation accuracy is highly dependent on the quality of the alignment: Chinese prepositions aligned to English prepositions are almost twice as accurate than the average Chinese preposition. Still, the change of domain and language have a critical negative impact on the neural model performance (a 40-50 pt. drop overall, and a 10-20 pt. drop on the subset of Chinese prepositions aligned to English prepositions). Moreover, the neural model accuracy is of the same order of magnitude as the MF-Prep baseline across all reported Chinese subsets - indicating that any complex pattern it might have detected in the training data was not as useful for Chinese predictions. The combined model (Neural + MF-Prep) performs similarly to both models, which is another indication that they behave similarly on Chinese.










