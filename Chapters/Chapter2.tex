% Chapter Template

\chapter{Preposition Supersenses Disambiguation} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Overview}
\textbf{Semantic Network of Adposition and Case Supersense} (SNACS), by (\cite{snacs}), is a labeling schema assigning semantic attributes to adpositions. Each adposition is assigned with a pair of \emph{role} and \emph{function} supersenses, from a bank of 50 labels called \emph{Preposition Supersenses} or \emph{PSS} \footnote{See section \ref{sec:snacsscheme} for a detailed description of the SNACS schema.}. These annotations provide an additional layer of semantic information regarding a sentence, which could be beneficial for downstream NLP applications. In this chapter, we present a neural PSS disambiguation model, trained and evaluated on the STREUSLE corpus of gold-standard PSS annotations. 

This chapter begins with a more formal description of the task as a pipline of two subtasks - (i) target identification (section \ref{sec:probform_ident}) - identifying the adpositions of a sentence requiring a PSS annotation, and (ii) disambiguation (section \ref{sec:probform_disam}) - deciding on the correct PSS annotation of an identified adposition. It follows with an in-depth description of the STREUSLE corpus used for training and evaluation (section \ref{sec:streusle}), and the heuristic process used for target identification (section \ref{sec:ident}). The following sections present the architecture of the neural PSS disambiguation model and data preprocessing steps (section \ref{sec:pssdisambg}), the training process (section \ref{sec:psstraining}), and the different model and evaluation settings used throughout the experiment (section \ref{sec:psssettings}).  Finally, the evaluation of the model compared to a set of baselines is presented and analysed (sections \ref{sec:pssbaselines}, \ref{sec:pssresults}, and \ref{sec:pssanalysis}).

\section{Problem Formulation} \label{sec:probform}
\subsection{Target Identification}\label{sec:probform_ident}
The target identification task deals with identifying the PSS-bearing units in a given sentence. A PSS bearing unit is not limited to just prepositions (despite the confusing terminology) - a PSS can also be applied to postpositions (\emph{"ago", "aside"}), cases (\emph{"Harry\textbf{'s} wand", "\textbf{his} wand"}) and many other phrases that impose a preposition-like behaviour (\emph{"in front of", "out of"})\footnote{A more thorough categorization of PSS bearing units is provided in section \ref{sec:snacsscheme}}.  The set of PSS bearing units is therefore comprised of non-overlapping, consecutive token sequences, as defined by the SNACS specification and annotation guide. 

Most PSS-bearing units are plain adpositions, and thus can be easily identified using POS labels or even a predefined adposition vocabulary. In many cases, however, lexicality alone is not enough to determine a preposition. The word \emph{to}, for instance, is considered a PSS-bearing preposition in \emph{"They worked \textbf{to} fix the issue."} (PURPOSE), but not considered one in \emph{"I need \textbf{to} leave."} (infinitive-clause), despite the similar structure. POS information is also not enough for a comprehensive identification - while 61.2\% of the units annotated in the STREUSLE corpus are adpositions according to gold POS annotation, 20.2\% are possessives, and 18.6\% belong
to other POS classes. The task of identifying the correct PSS units is therefore not trivial as one might expect. This work makes use of a PSS identification heuristic by (\cite{snacs}), described in section \ref{sec:ident}.

\subsection{Supersense Disambiguation}\label{sec:probform_disam}

The identified units are to be assigned with a \emph{Scene Role} and \emph{Function} PSS, both from the same bank of labels defined in the SNACS specification (section \ref{sec:construal}). Role and Function labels tend to collide, as demonstrated by the STREUSEL corpus, where 67.8\% of instances are annotated with the same label for both. The neural PSS disambiguation model presented in section \ref{sec:pssdisambg} jointly predicts both labels.

\section{The STREUSLE corpus}\label{sec:streusle}

\begin{figure}
\begin{exe}\raggedright
\ex This cottage is a charming homely, friendly, place \p{to}/\rf{Characteristic}{Purpose} stay.
\ex I came in \p{to}/\psst{Purpose} get a nice gift \p{for}/\rf{Recipient}{Direction} \p{my}/\rf{SocialRel}{Gestalt} wife.
\ex He worked \p{on}/\psst{Theme} it right \p{on}/\psst{Locus} the back \p{of}/\psst{Whole} \p{my}/\psst{Possessor} car.
\end{exe}
\caption{Annotated sentences from the STREUSLE corpus. PSS labels are annotated as \rf{Role}{Function}, or a single lable when they collide}
\label{fig:streuslesamples}
\end{figure}


The STREUSLE corpus of PSS annotated sentences (\cite{snacs}) was constructed by applying the SNACS scheme to a collection of online consumer reviews taken from the English Web Treebank (\cite{ewtb}). Standard train/dev/test splits are used, consisting of 2,723/554/535 sentences and 4,522/453/480 annotated targets respectively. 305 (5.5\%) of units  are multiword expressions, and for a total of 3,702 (67.8\%) units the Role and Function labels collide. The corpus contains 249 different types of PSS-bearing units, from which 149 are mutiword expressions. The most frequent PSS label in the corpus is LOCUS - annotated for Role (11.6\%), Function (14.2\%), and both (10.4\%).

Sentences in the English Web Treebank corpus are equipped with manually -annotated syntactic information, in the form of tokenization, lemmatization, and a full Universal Dependencies parse tree. The STREUSLE corpus is bundled with this information, which from here on out is referred to as \emph{gold syntax}. Similarly, the set of manually-identified PSS bearing units are refered to as \emph{gold identification}. Example sentences from the corpus, along with their PSS annotations, are available in figure \ref{fig:streuslesamples}.

\section{Target Identification Heuristic} \label{sec:ident}

A fully automated PSS annotation system requires the ability to automatically identify PSS-bearing units, without relying on human annotated targets such as in the STREUSLE corpus. This work makes use of a PSS target identification heuristic by (\cite{snacs}), jointly developed along with the neural PSS disambiguator presented in this work in a collaborative manner, to facilitate a fully automated PSS annotator. 

As a preprocקssing step, the algorithm makes use of the training portion of the STREUSLE corpus to construct a set of lexical lists:
\begin{enumerate}
    \item A blacklist of non PSS-bearing multiword expressions containing preposition tokens (e.g., \emph{take care of}).
    \item A whitelist of PSS-bearing multiword expressions.
    \item A whitelist of PSS-bearing single word expressions.
    \item A whitelist and a blacklist of governors for PSS-bearing and non PSS-bearing instances of \emph{to} respectively, 
\end{enumerate}

For (4), the algorithm makes use of the UD parse-tree. List (1), (2) and (3) are enriched with a set of manually identified lexical terms. The identification process is then performed as follows: sentences are first scanned for multiword expressions using lists (1) and (2). From the remaining segments, single-word candidates are identified by matching a high-recall set of POS tags,
then filtered through 5 different heuristics for adpositions, possessives, subordinating conjunctions,
adverbs, and infinitivals. Most heuristics use a combination of rules involving syntax information and the use of list (3) for filtering. The infinitival heuristic also makes use of the learned whitelist and blacklist described in (4).

\begin{table}[t]\centering\small
\begin{tabular}{lccc}
Syntax & P & R & F \\
\toprule
gold & 88.8 & 89.6 & 89.2 \\
auto & 86.0 & 85.8 & 85.9 \\
\end{tabular}
\caption{Target identification results on the STREUSLE test set, when using gold vs. automatically parsed syntax information.}
\label{tab:targetid}
\end{table}

Table \ref{tab:targetid}
reports the precision, recall, and F-score (P/R/F) of
the target identification heuristic on the test portion of the STREUSLE corpus. This evaluation compares the use of gold vs. automatically parsed syntax information, showing a slight decrease of quality when using the latter. 47 of 54 false positives (87\%) can be ascribed to tokens that are part of a (non-adpositional or larger adpositional) multiword expression. 9 of the 50 false negatives (18\%) are rare multiword expressions not
occurring in the training data, and there are 7 partially identified ones, which are counted as both
false positives and false negatives.

\section{PSS Disambiguation Model} \label{sec:pssdisambg}

\begin{figure}
    \centering\small
    \includegraphics[width=\textwidth]{Figures/pssmodel.pdf}
    \caption{The full neural PSS disambiguation model. Tokens are fed into stacked BiLSTM layers. For each PSS-bearing unit, a feature vector is constructed and fed into two MLP-Softmax layers, predicting label probabilities for Role and Function}
    \label{fig:pssmodel}
\end{figure}

The neural PSS disambiguation model presented in this work is a multilayer perceptron (MLP) stacked on top of a multilayered BiLSTM. Such architectures have proven to be successful for a variety of NLP tasks (\cite{kiperwasser16simple, elmo}). The model architecture is largely comparable to that of (\cite{gonen16semi}), who experimented with a coarsened version of STREUSLE. The main difference is their use of unlabeled multilingual datasets to improve prediction by exploiting the differences in preposition ambiguities across languages. The model makes use of syntactic information in the form of tokenization, lemmatization, NER annotation, and a full Universal Dependencies parse tree, all which are assume to be given as inputs in addition to the actual sentence and target words.  The full model architecture is illustrated in figure \ref{fig:pssmodel}. 

Each sentence word $w_i$ is first converted into a vector using pretrained word embeddings, and a vocabulary-sized lookup table for an internal representation of each word. We experiment with contextualized (ELMo) and non-contextualized (SkipGram) pretrained embeddings\footnote{See section \ref{section:wordembeddings} for more details regarding each embedding method.}. For ELMo, the pretrained model from the AllenNLP python module (\cite{allennlp}) is used. For skip gram, the pretrained FastText vectors are used (\cite{fasttext}). The full word embedding $e_i$ is therefore a concatenation of three vectors: the pretrained embeddings of the word $e^{word}_i$ and lemma $e^{lemma}_i$, and the internal word representation $e^{internal}_i$. 

$$ e_i = [e^{word}_i;e^{lemma}_i;e^{internal}_i] $$ 

Hyperparameters control which pretrained embeddings to use (ELMo/SkipGram), and in the case of fastText, whether the pretrained vectors remain fixed or get updated during training. A single ELMo layer in the pretrained model consists of 1000 elements, whereas for fastText the vectors are of size 300. The dimension of the internal representation (if used) is also determined by a hyperparameter. The size of the full vector is therefore largely dependent of the choice of hyperparameters. A $NONE$ label is used to represent words lacking a pretrained representation, both in training and at test time.

The resulting word embeddings $e_1, .. ,e_n$ are then fed into a multilayered BiLSTM, yielding contextualized word representations $c_1, .. ,c_n$. The number of LSTM layers, the dimension of the LSTM output, and whether the bidirectional variant is used, are all controlled by hyperparameters.

$$ c_1, ... , c_n = Multilayerd\_BiLSTM(e_1, ... ,e_n)$$

A vector $u_{start:end}$ is then constructed for an identified PSS-bearing target span $(w_{start}, .., w_{end})$, using the corresponding contextualized vectors and the auxiliary syntactic information (collectively denoted here as \emph{syntax}). The exact structure of this vector, which is refereed to as the Preposition Feature Vector, is explained in section \ref{sec:enrichedvec}. 

$$u_{start:end} = Build\_Prep\_Feature\_Vec(c_1, ... ,c_n, start, end, syntax)$$

Each feature vector $u$ is then fed into two separate MLPs, followed by separate Softmax layers that
yield the predicted probabilities for the Role and
Function labels:

$$ p^{Role}_{start:end} = Softmax(MLP_{Role}(u_{start:end})) $$
$$ p^{Function}_{start:end} = Softmax(MLP_{Function}(u_{start:end})) $$

$p^{Role}_{start:end}$ and $p^{Function}_{start:end}$ are then the corresponding predicted probability distributions for Role and Function labels respectively. The number of MLP layers and dimensions, as well as the activation functions used by the MLP layers, are all considered model hyperparameters. 

\subsection{Constructing the Preposition Feature Vector} \label{sec:enrichedvec}
The preposition feature vector is obtained by constructing and concatenating feature vectors for the preposition, and its object and governor headwords. The object and governor of a preposition are determined in a heuristic process (described in section \ref{sec:govobj}), which also emits the preposition's lexical category - a syntactic label indicating whether the preposition is
predicative/stranded/possesive/subordinating/none of these. 

A single feature vector for a word $w_i$ is obtained by concatenating its contextualized representation $c_i$, with the embeddings of its POS, UD dependency and NER labels. The preposition vector is also concatenated with the embedding of its lexical category, and a binary indicator of whether either of the two tokens following the preposition is capitalized. In the case of multiword prepositions, the first token is used. All labels are embedded using randomly initialized lookup tables, with dimensions determined by hyperparameters.

\subsection{Extracting the Governor and Object of a Preposition} \label{sec:govobj}

The governor and object of a preposition may provide a great deal of information regarding the semantics of the preposition. The heuristic process used for identifying the governor and object of a preposition, by (\cite{snacs}) was developed as part of a collaborative effort on the task of PSS disambiguation. In addition to identification, it also sorts the prepositions into 5 lexical categories: stranded (\emph{"The book I told you \textbf{about}"}), predicative (\emph{"The dog is \textbf{under} the table"}), possessive (\emph{"The dog\textbf{'s} food"}), subordinating (\emph{"I will arrive \textbf{before} sunset"}), or none of these. The heuristic applies a set of rules using the UD parse tree and POS labels: first, the UD dependency label of the preposition is examined. For prepositions with \emph{case} or \emph{mark}, the parent and grandparent of the preposition in the UD tree are usually the object and governor, respectively. Prepositions with different labels are typically not attached to an object, with their parent being the governor.  The \emph{nmod:poss} UD label is an indication of a possessive preposition. The heuristic process then attempts to handle exceptions to the above, by explicitly matching against patterns corresponding to stranded prepositions and copular constructions.  

\section{Training the Model} \label{sec:psstraining}

The model was trained using the cross-entropy loss function. More specifically, the loss of a single identified PSS target is defined as the negated log-sum of the probabilities assigned to the correct labels: 
$$ Loss(x, y) = - log(p_r) - log(p_f) $$

Where $p_r$ and $p_f$ are the probabilities assigned by the model to the correct Role and Function labels ($y$) for the given sentence and identified PSS unit ($x$).

Model parameters are initialized randomly using the Xavier initialization (\cite{glorot10xavier}). Inverted dropout was applied for the LSTM and MLP layers. Training was performed using SGD with Backpropagation for 130 epochs, and a minibatch of size 20. To avoid overfitting, the model parameters corresponding to the best performing epoch on the development set are selected. The dropout probabilities, SGD learning rate and learning rate decay factor are all hyperparameters. The model was implemented with the DyNet library (\cite{dynet}).

\subsection{Hyperparameter Tuning}

The development section of the STREUSLE corpus was used to tune hyperparameters using a random search process. Hyperparameters are iteratively sampled and used to train a model in the process described above. The best performing set of hyperparameters is selected. Hyperparameters were used in this manner for tuning the structure of the network (number of layers, dimensions, activations), tuning the optimizer (learning rate and learning rate decay), and feature selection (e.g. which syntactic information and word embeddings to use). The complete list of hyperparameters is available in table (\ref{tab:hyperparams}).

\section{Experiment Settings} \label{sec:psssettings}

The aformetioned PSS dismabiguation model heavily relies on two preprocessing steps: syntactic information extraction (tokenization, lemmatization, UD parsing, NER labeling, and so on), followed by PSS target identification (section \ref{sec:probform_ident}). The required syntactic information - with the exception of NER labels - is entirely available through the gold standard annotations of the STREUSLE corpus, referred to here as \textit{gold-syntax}. To better handle real-life scenarios where such syntactic information is not available, the model is also evaluated using an automatically parsed syntactic information instead, by applying the CoreNLP parser (\cite{manning14stanford}) to the entire corpus. The settings in which such syntactic information is used are referred to as \textit{auto-syntax}. Each setting is using the corresponding syntactic information source throughout the entire process of target identification, training and testing. 

We take a similar approach with the target identification step: the model is evaluated using the gold-standard identifications in the STREUSLE corpus (\textit{gold-id} settings), and also using the automatic identification heuristic from section \ref{sec:ident} (\textit{auto-id} settings).

The model is evaluated for each of the four combinations of \textit{gold-id/auto-id} and \textit{gold-syntax/auto-syntax}. The training and tuning processes described above are applied to each setting individually.

\section{Baselines} \label{sec:pssbaselines}

In addition to the neural model, we also evaluate the following two PSS disambiguation baselines:

\begin{itemize}
    \item \textbf{Most Frequent Label (MF)} - always predicting the most frequent PSS label seen in the training data.
    \item \textbf{Most Frequent Label per Preposition (MF-Prep)} - predicting the most frequent PSS label for the specific (lemmatized) preposition, according to the training data.
\end{itemize}

Both baselines are evaluated and compared with the full model for each of the four experiment settings. 

\section{Results} \label{sec:pssresults}

\begin{table*}[]\label{tab:pssresults}
    \newcommand{\score}[2]{#1{\tiny ($\pm$#2)}}
    \newcommand{\acc}[1]{\multicolumn{3}{c|}{\textcolor{gray}{\rule[2pt]{0.5in}{0.5pt}} #1 \textcolor{gray}{\rule[2pt]{0.5in}{0.5pt}}}}
    \newcommand{\accl}[1]{\multicolumn{3}{c}{\textcolor{gray}{\rule[2pt]{0.5in}{0.5pt}} #1 \textcolor{gray}{\rule[2pt]{0.5in}{0.5pt}}}}
    \newcommand{\accs}[1]{\multicolumn{3}{c|}{\textcolor{gray}{\rule[2pt]{0.3in}{0.5pt}} #1 \textcolor{gray}{\rule[2pt]{0.3in}{0.5pt}}}}
    \newcommand{\accsl}[1]{\multicolumn{3}{c}{\textcolor{gray}{\rule[2pt]{0.3in}{0.5pt}} #1 \textcolor{gray}{\rule[2pt]{0.3in}{0.5pt}}}}

	\centering\footnotesize
    \setlength{\tabcolsep}{1pt} % Default value: 6pt
    \renewcommand{\arraystretch}{1} % Default value: 1
	\begin{tabular}{@{}ccc<{\hspace{5pt}}ccc|ccc|ccc@{}}
% 		\toprule
                          & &          & \multicolumn{3}{c}{\textit{Role}} & \multicolumn{3}{c}{\textit{Func.}} & \multicolumn{3}{c}{\textit{Full}}              \\
                          & \textbf{Syntax} & \textbf{ID}       & P                                 & R                                  & F    & P    & R    & F    & P    & R    & F    \\
		\midrule
        \multirow{2}{*}{MF} & \multirow{2}{*}{N/A} & gold & \acc{ 15.8 } & \acc{ 19.8 } & \accl{ 14.8 } \\
          &   & auto & 14.3 & 14.4 & 14.3 & 16.5 & 16.7 & 16.6 & 13.2 & 13.3 & 13.3 \\
        \midrule
        \multirow{2}{*}{MF-Prep} & \multirow{2}{*}{N/A} & gold & \acc{ 40.6 } & \acc{ 53.3 } & \accl{ 37.9 } \\
          &   & auto & 37.0 & 37.3 & 37.1 & 49.8 & 50.2 & 50.0 & 34.3 & 34.6 & 34.4 \\        \midrule
        \multirow{4}{*}{Neural}        & \multirow{2}{*}{gold} & gold & \accs{\score{ 79.5 }{ 0.6 } }& \accs{\score{ 86.8 }{ 0.9 } }& \accsl{\score{ 75.7 }{ 0.6 } }\\
         &   & auto & \score{ 68.6 }{ 1.0 } & \score{ 69.2 }{ 1.0 } & \score{ 68.9 }{ 1.0 } & \score{ 75.5 }{ 0.6 } & \score{ 76.1 }{ 0.6 } & \score{ 75.8 }{ 0.6 } & \score{ 65.2 }{ 1.0 } & \score{ 65.7 }{ 1.0 } & \score{ 65.4 }{ 1.0 } \\
         \cmidrule{2-12}                & \multirow{2}{*}{auto} & gold & \accs{\score{ 77.5 }{ 1.2 } } & \accs{\score{ 85.9 }{ 0.9 } }& \accsl{\score{ 74.0 }{ 1.4 } }\\
         &   & auto & \score{ 66.4 }{ 0.8 } & \score{ 66.2 }{ 0.9 } & \score{ 66.3 }{ 0.9 } & \score{ 74.9 }{ 0.6 } & \score{ 74.8 }{ 0.5 } & \score{ 74.8 }{ 0.6 } & \score{ 63.6 }{ 0.6 } & \score{ 63.5 }{ 0.6 } & \score{ 63.6 }{ 0.6 } \\
         \midrule
% 		\bottomrule
	\end{tabular}
	\caption{\label{tab:overall} Overall performance of SNACS disambiguation systems on the test set. Results are reported for the role supersense ({\it Role}), the function supersense ({\it Func.}), and their conjunction ({\it Full}). The average and standard deviation for 5 different training instances are reported. All figures are percentages.}
\end{table*}


The precision, recall, and $F_1$ score ($P/R/F$) for the best performing models and baselines on the test set, in each setting, are listed in table \ref{tab:pssresults}. The corresponding model hyperparameters in each setting are listed in table \ref{tab:hyperparams}. For the \textit{gold-id} settings, where $P = R = F$, a single accuracy score is reported. Due to the relatively small size of samples in the test set (480 prepositions), the scores may vary slightly but noticeably between one instance of a trained model to another (trained using different random seeds). For that reason, the average of 5 different training instances is reported, along with the standard deviation.

The neural systems outperform the most-frequent baselines by a large margin for both Role and Function label prediction across all settings. Another observation is that Function prediction is consistently more accurate then Role prediction, by a margin of roughly 10 points. This is probably due to the reduced ambiguity of functions compared to roles (as can be observed in the most-frequent baseline scores). The use of automatic syntax consistently decreases accuracy scores across all systems (a $\sim$3 point gap). In fact, the best performing model in the auto-syntax/auto-id setting does not use syntax at all (see table \ref{tab:hyperparams}). The target identification accuracy (table \ref{tab:targetid}) imposes a performance bottleneck for the auto-id settings, as can be observed by a consistent $\sim$10 point drop. 

\section{Analysis} \label{sec:pssanalysis}

\subsection{Implication of Word Embeddings and Syntax}

\begin{table}\label{tab:embdsyn}
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5}
\newcommand{\score}[2]{#1{ ($\pm$#2)}}
\newcommand{\acc}[1]{\multicolumn{3}{c|}{\textcolor{gray}{\rule[2pt]{0.5in}{0.5pt}} #1 \textcolor{gray}{\rule[2pt]{0.5in}{0.5pt}}}}
\newcommand{\accl}[1]{\multicolumn{3}{c}{\textcolor{gray}{\rule[2pt]{0.5in}{0.5pt}} #1 \textcolor{gray}{\rule[2pt]{0.5in}{0.5pt}}}}
\centering
\begin{tabular}{|c|cc|cc|}

\hline
\multirow{2}{*}{\backslashbox{Embeddings}{Syntax}}   & \multicolumn{2}{|c|}{\textbf{w/o Syntax}} & \multicolumn{2}{|c|}{\textbf{w/ Syntax}}  \\
& Role & Func. & Role & Func. \\ \hline
\textbf{fastText} &  \score{ 41.1 }{ 0.5 }     &  \score{ 54.0 }{ 0.5 } &  \score{ 70.9 }{ 1.6 }     &  \score{ 82.5 }{ 0.8 } \\ \hline
\textbf{ELMo} &  \score{ 78.0 }{ 1.7 }     &  \score{ 86.4 }{ 1.3 } &  \score{ 79.5 }{ 0.6 }     &  \score{ 86.8 }{ 0.9 } \\ \hline
\end{tabular}
\caption{Average role and func. prediction accuracy scores for the 4 models variations corresponding to the selection of word embeddings and use of syntax, in the gold-id/gold-syntax setting.}

\end{table}

Contextualized word embeddings, and ELMo in particular, have consistently been shown to outperform non-contextualized embedding methods such as SkipGram and GloVe in many kinds of prediction tasks (\cite{elmo, nelson}). (\cite{nelson}) in particular demonstrate that for PSS disambiguation, a simple linear model over the ELMo representations of prepositions significantly outperforms complex and feature-rich models that make use of non-contextualized embeddings and auxiliary syntactic information. In fact, one of their baselines is a previous version (\cite{snacs}) of the PSS model described in this chapter, which used pretrained, non-contextualized  Word2Vec  embeddings\footnote{The Word2Vec model was pretrained on the Google News corpus.}. The results presented in this chapter are consistent with these findings, as can be seen by the fact that ELMo embeddings yield the best scores across all settings.

As it becomes apparent that ELMo embeddings are able to capture syntactic and semantic information, it is unclear how useful is auxiliary syntactic information when ELMo embeddings are also provided. To assess this, we evaluate 4 variations of the model, corresponding to the choice of word embeddings (ELMo/fastText) and the presence of auxiliary syntax. The rest of the hyperparameters (activations, number of layers, layer dimensions, etc.) are tuned separably for each variation. The accuracy scores for each variation in the gold-id/gold-syntax setting are reported in table \ref{tab:embdsyn}.

As expected, using the non-contextualized fastText embeddings alone yields similar results to the MF-Prep baseline, as the embeddings represent type level prepositions. Adding syntactic signals on top of fastText embeddings introduces the model with crucial contextualized information, and as a result yielding a $\sim$30 point gain in accuracy. Consistent with the findings of (\cite{nelson}), a system relying solely on contextualized ELMo embeddings (without syntactic signals) performs significantly better then a fastText+syntax based system (a 4-6 point gap). Interestingly, supplying syntactic signals to an ELMo based system yields a small, possibly insignificant gain in performance. 

\subsection{Errors Across the SNACS Hierarchy}

\begin{table}[]
    \setlength{\tabcolsep}{10pt} % Default value: 6pt
    \renewcommand{\arraystretch}{1.5}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
            & Labels & Role & Func.\\ \hline
         Depth-4 (Exact) & 47 & 79.8 & 86.0\\ \hline
         Depth-3 & 43 & 80.2 & 86.3 \\ \hline
         Depth-2 & 26 & 83.3 & 89.8 \\ \hline
         Depth-1 & 3 & 89.6 & 94.2 \\ \hline
          
    \end{tabular}
    \caption{Accuracy scores (single model, gold identification and syntax) on the test set (480 tokens) with different levels of hierarchy coarsening of its output. “Labels” refers to the number of labels in the training set after coarsening.}
    \label{tab:coarsening}
\end{table}

The PSS label hierarchy can be used to compute a more fine-grained measurement of disambiguation performance.  Specifically, the accuracy of predicted labels is evaluated when they are coarsened post hoc by moving up the hierarchy to a specific depth, with depth-1 representing the coarsening of the labels into the 3 root labels. The accuracy for each depth in the gold-syntax/gold-id setting is reported in table \ref{tab:coarsening}. These
results show that the classifiers often mistake a label for another that is nearby in the hierarchy.

For \textit{role}, around 25\% of the errors are when the correct label is either \psst{OrgRole}, \psst{Topic}, or \psst{Locus}.  \psst{OrgRole} is often confused with  \psst{Possessor}; \psst{Topic} with \psst{Beneficiary} and \psst{Circumstance}; and \psst{Locus} with the more specific \psst{Goal} and \psst{Source}. 
\psst{Topic} and \psst{Locus} are also a large source of errors for \textit{function}, as well as \psst{ComparisonRef} and \psst{Theme}, which together account for 35\% of the errors.


