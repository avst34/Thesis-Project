% Chapter Template

\chapter{Preposition Supersenses Disambiguation} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Overview}
\textbf{Semantic Network of Adposition and Case Supersense} (SNACS) (todo: reference), by Schneider et al 2017, is a labeling schema assigning semantic attributes to adpositions. Each adposition is assigned with a pair of \emph{role} and \emph{function} supersenses, from a bank of 50 labels called \emph{Preposition Supersenses} or \emph{PSS} \footnote{See section (todo ref) for a detailed description of the SNACS schema.}. These annotations provide an additional layer of semantic information regarding the sentence, which could be beneficial for downstream NLP applications. In this chapter, we present a neural PSS disambiguation model, trained and evaluated on the STREUSLE corpus of gold-standard PSS annotations. (todo - ref to ACL paper)

This chapter begins with a more formal description of the task as a pipline of two subtasks - (i) target identification (section \ref{sec:probform_ident}) - identifying the adpositions of a sentence requiring a PSS annotation, and (ii) disambiguation (section \ref{sec:probform_disam}) - deciding on the correct PSS annotation of an identified adposition. It follows with an in-depth description of the STREUSLE corpus used for training and evaluation (section \ref{sec:streusle}), and the heuristic process used for target identification (section \ref{sec:ident}). The following sections present the architecture of the neural PSS disambiguation model and data preprocessing steps (section \ref{sec:pssdisambg}), the training process (section \ref{sec:psstraining}), and the different model and evaluation settings used throughout the experiment (section \ref{sec:psssettings}).  Finally, the evaluation of the model compared to a set of baselines is presented and analysed (sections \ref{sec:pssbaselines}, \ref{sec:pssresults}, and \ref{sec:pssanalysis}).




\section{Problem Formulation} \label{sec:probform}
\subsection{Target Identification}\label{sec:probform_ident}
\subsubsection{*Informal and formal definition}
\subsubsection{*Why this is not trivial}
\subsection{Supersense Disambiguation}\label{sec:probform_disam}
\subsubsection{*Informal and formal definition}

\section{The STREUSLE Dataset}\label{sec:streusle}
\subsubsection{*Overview}
\subsubsection{*Characteristics and notable stats}
\subsubsection{*Samples}

\section{Target Identification Heuristic} \label{sec:ident}
\subsubsection{*Credit}
\subsubsection{*Brief description}
\subsubsection{*Evaluation}

\section{PSS Disambiguation Model} \label{sec:pssdisambg}
\subsubsection{*General Architecture}
\subsubsection{* - Embeddings are fed to Stacked BiLSTM}
\subsubsection{* - A vector is created for each preposition using LSTM output and preprocessed data}
\subsubsection{* - The vector is fed through a FF-Network followed by Softmax}
\pagebreak
\subsection{Constructing the Enriched Preposition Vector}
\subsubsection{*LSTM vec is concatenated with}
\subsubsection{*GOV/OBJ vecs with GOVOBJ\_CONFIG}
\subsubsection{*POS of PREP, GOV, OBJ}
\subsubsection{*UD dep encoding of PREP, GOV, OBJ}
\subsubsection{*NER encoding of GOV, OBJ}
\subsubsection{*Other features}

\section{Training} \label{sec:psstraining}
\subsubsection{*Loss function}
\subsubsection{*Trainer, Initialization, Dropout}
\subsubsection{Hyperparameter Tuning}

\section{Settings} \label{sec:psssettings}
\subsubsection{*Automatic/Gold Target Identification}
\subsubsection{*Automatic/Gold Preprocessing}
\subsubsection{*WORD2VEC/ELMo Word Embeddings}

\section{Baselines} \label{sec:pssbaselines}
\subsubsection{*Describe MF baselines}

\section{Results} \label{sec:pssresults}

\section{Analysis} \label{sec:pssanalysis}
\subsection{Ablation Experiments?}
\subsection{Error Analysis}
\subsubsection{*Confusion matrix}
\subsubsection{*Cross-Hierarchy Errors}

\section{Discussion?}

\section{Future Work}

