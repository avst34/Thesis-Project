% Chapter Template

\chapter{Preposition Supersenses for  Prepositional Phrase Attachment} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


%-----------------------------------
%	SECTION 1
%-----------------------------------
\section{Overview}

\section{PPA work by Belinkov et. al 2014}
\subsubsection{*Overview, relevance to this work}
This section introduces the PP-Attachment work by (Belinkov et al. 2014), which we use in our research. We incorporate PSS features onto their top-performing model, measuring the impact on accuracy. 

\subsubsection{*Intro to Belinkov work}
(Belinkov et al. 2014 todo) is perhaps the first to present a set of neural PP-Attachment models that make use of pretrained word embeddings. Their top performing model achieved state-of-the-art results, surpassing any previous attempts at this task. They experiment with different sets of word embeddings (skip-gram based and dep-gram based todo: ref), using a PP-Attachment dataset they constructed using the Penn Treebank for English and the CATiB dependency treebank (Habash and Roth, 2009) for Arabic.    

\subsubsection{* -- Isolated PPA models}
\subsubsection{* -- Incorporated to an existing parser}
The models presented operate in an isolated manner - solving PP-Attachment alone. Instead of the typical V-NP-P-NP2 formulation, which as explained in section todo was shown to be problematic, they took the more realistic approach of allowing multiple head candidates. They do, however, compare against full-scale parsers, and integrate their model into an existing one - the RGB parser (todo: ref) - resulting with an improvement of parsing accuracy in both English and Arabic when compared to using the unmodified RGB parser.

\subsubsection{* -- Model architecture in brief}
Their model architecture consists of a parameterized neural scoring function, which assigns a score to a given attachment candidate, given the corresponding word embeddings of the preposition, its child (the object) the head candidate, and their corresponding part-of-speech and WordNet, FrameNet attributes. The parameters of the scoring function are learned during training, and at inference the highest scoring candidate is chosen. They experiment with a few different scoring functions, each consists of a slightly different neural network architecture. The variations differ mostly by: (1)  the input - as one variation does not take the preposition as an input, only the head candidate and the object of the preposition; (2) the number of network layers - a single layer or two layers, and (3) the parameterization - for instance, one variation uses a different set of parameters based on the distance between the head candidate and the preposition. They also experiment with two kinds of pre-trained word embeddings: one based on the Skip-Gram model (todo: ref) and a dependency based model following (todo: ref), reporting significant accuracy boost when using the latter. Allowing the word embeddings to update during training was also reported to increase disambiguation accuracy significantly. 

\subsection{Datasets}
\subsubsection{*Structure, size and other notable stats}
\subsubsection{*Extraction process}

\subsection{Model}
\subsubsection{*Model framework}
The following section assumes that sentence words are represented using a column vector of dimension $n$. Section todo describes how such representation is constructed. 

As described in section todo, the model consists of a scoring function that assigns a score to each attachment candidate. For inference, the highest scoring candidate is selected. The detection of a preposition and its attachment candidates is done in a rule-based process as a preprocessing step, which is used for building the datasets used for training and evaluation (see section todo).

The scoring function $s(c, z, h, \theta)$ takes a preposition $z$ with child $c$, and an attachment candidate $h$ ($\theta$ is the set of learned model parameters). The score is defined to be a linear function on a vector composed from $c, z, h$:

$$s(c,z,h,\theta) = w * compose\_p(c,z,h, \theta’)$$

Where $w \in \mathbb{R}^n$ is a parameter, and $\theta'$ is the set of all parameters, excluding $w$. $compose\_p$ transforms $c, z, h$ into a single column vector $p \in \mathbb{R}^n$ using parameters in $\theta’$.  The score is obtained by taking the dot product of $w$ and $p$. 

\subsubsection{*Score function variations}
Several variations of $compose\_p$ are suggested, each corresponding to a different model variant. The top performing variant - Head-Prep-Child-Dist (HPCD) - is the one used in this research, and the only one described in detail. The rest of the variants are mainly simpler versions of HPCD.

\subsubsection{The HPCD scoring function}
The HPCD version of $compose\_p$ is defined as follows:

   $$ dist = min(word\_distance(head, prep), MAX\_DIST) $$
   $$ p_1 = tanh(W_{dist}[z;c] + b_{dist})   $$
   $$ p_2 = tanh(W_{dist}[h;p_1] + b_{dist}) $$

Where $h, z, c \in \mathbb{R}^n$ are vectors for the head, the preposition, and its child respectively; $p_1, p_2 \in \mathbb{R}^n$ are the output vectors for the first and second layers respectively;
word\_distance(head, prep) is the number of words between the preposition and the head candidate; and $ W_{dist} \in \mathbb{R}^{n \× 2n}, b_{dist} \in \mathbb{R} $.

Namely, the HPCD scoring function is composed of a two-layered feed-forward neural network. The first layer takes the vectors for the preposition and its child; the second layer takes the output vector from the first layer and the vector for the head candidate. The result is the output of the second layer. The HPCD variant takes into account the distance between the head candidate and the preposition (i.e the number of words between them), by using different neural network parameters for each distance, up to a predefined threshold (MAX\_DIST). For any distance larger than the threshold, the parameters corresponding to threshold value are used.

\subsection{Word Vector Representation}

The word vectors for the head, preposition, and child words are constructed using pretrained word embeddings, and extended using auxiliary syntactic information and structured knowledge resources such as WordNet and VerbNet. They experiment with two kinds of SkipGram based word embedding models. The first is the standard SkipGram model, where the context of a word is defined as the following and preceding words in the sentence. The second is a variation of standard SkipGram by Bansal et al. (2014), where instead the context of a word is defined using the head and child words according to the dependency tree of the given sentence. These word embeddings (referred to as \textit{syntactic} embeddings) are expected to better capture dependency related information and therefore be relevant for PP-Attachment disambiguation. As their experiments show, the syntactic vectors indeed yield a significant boost in accuracy. The same 100-dimensional pretrained syntactic vectors are also used in this work.





\subsubsection{*Word Embedding Variations}
\subsubsection{*Vector Enrichment (WordNet, VerbNet, etc)}
\subsubsection{*Full Vector Outline}
\subsection{Training}
\subsection{Results}
\subsection{Conclusion}
\pagebreak

%-----------------------------------
%	SECTION 2
%-----------------------------------
\section{Datasets}
\subsection{World Stree Journal based dataset}
\subsubsection{*PSS enrichment of WSJ}
\subsection{STREUSLE based dataset}
\subsubsection{*Extracting PPA cases from STREUSLE}

\section{Models}
\subsection{Baseline Model: HPCD}
\subsection{HPCD extended with PSS features}

\section{Training}
\subsubsection{*Loss function and trainer remain unchanged}
\subsubsection{*Process of training on both datasets}
\subsubsection{*Hyperparameter tuning}

\section{Results}
\subsubsection{*Compare HPCD and extended HPCD on each setting and dataset}

\section{Analysis}
\subsection{Error Analysis}
\subsubsection{*Errors shared between both models}
\subsubsection{*Errors due to wrong PSS? (sampling)}

\section{Conclusions}
