% Chapter Template

\chapter{Preposition Supersenses for  Prepositional Phrase Attachment} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


%-----------------------------------
%	SECTION 1
%-----------------------------------
\section{Overview}

The problem of \textbf{Prepositional-Phrase Attachment (PP-Attachment)} \footnote{The PP-Attachment problem is described in detail in section \ref{sec:ppatt}}, a well-known sentence parsing ambiguity, is the task of deciding on the correct attachment of a prepositional phrase to its head word. The work presented in this chapter attempts to examine how beneficial the information provided by PSS labels (section \ref{sec:pss}) is to PP-Attachment disambiguation. An existing PP-Attachment model is modified to take PSS information as input. Both the original and modified model variations are evaluated on several datasets, expecting to see a performance increase with the latter. 

As demonstrated in example \ref{ex:_ppatt}, different PP-Attachment candidates represent different semantic interpretations of a sentence, which may correspond to a different PSS labeling of the preposition. To facilitate the motivation behind using PSS information for PP-Attachment, consider sentences (1) and (2) from the example:

\begin{enumerate} \label{ex:_ppatt}
    \item[1.] \emph{"I am reading a book \textbf{with} a hard cover"}
    \item[2.] \emph{"I am reading a book \textbf{with} my children"}
\end{enumerate}

In (1), the correct head attachment for \textit{“with”} is \textit{"book"}, and the corresponding PSS label is \psst{Characteristic}. In (2), the correct correct head attachment is \textit{"reading"} and the PSS label is \psst{Accompanier}. The correspondence between the head attachment and the PSS label provides evidence to believe that a PP-Attachment disambiguator might benefit from using PSS information. 

The PP-Attachment model by \cite{hpcd} is presented in section \ref{sec:ppatt-belinkov}, along with its PSS-bearing variation (section \ref{sec:hpcd_pss}). The models are trained and evaluated using two datasets based on the STREUSLE corpus, and the Penn Treebank (section \ref{sec:hpcd_pss}). The experiment setup, results and analysis are presented in sections \ref{sec:ppatt_setup}, \ref{sec:ppatt_results}.

\section{PP-Attachment work by \cite{hpcd}} \label{sec:ppatt-belinkov}

\begin{figure}[h]
    \centering\small
    \includegraphics[width=\textwidth]{Figures/hpcd.pdf}
    \caption{An outline of the HPCD PP-Attachement model by \cite{hpcd}.}
    \label{fig:hpcd}
\end{figure}

\cite{hpcd} are perhaps the first to present a set of neural PP-Attachment models that make use of pretrained word embeddings. Their top performing model achieved state-of-the-art results, surpassing any previous attempts at this task. They experiment with different kinds of word embeddings methods, using a PP-Attachment dataset they constructed from the Penn Treebank  (\cite{ptb}) for English and the CATiB dependency treebank (\cite{habash09mada}) for Arabic. In this work, PSS features are incorporated onto their top-performing model, expecting a positive impact on accuracy. 

The models presented operate in an isolated manner - solving PP-Attachment alone. Instead of the typical V-NP-P-NP2 formulation, which as explained in section \ref{sec:ppatt_isolated} was shown to be problematic, they took the more realistic approach of allowing multiple head candidates. They do, however, compare against full-scale parsers, and integrate their model into an existing one - the RGB parser (\cite{lei-etal-14-low}) - resulting in an improvement of parsing accuracy for both English and Arabic when compared to using the unmodified RGB parser.

Their model architecture consists of a parameterized neural scoring function, which assigns a score to an attachment candidate, given the corresponding word embeddings of the preposition, its child (the object), the head candidate, and their corresponding Part-of-Speech tags and WordNet/VerbNet attributes. The parameters of the scoring function are learned, and at inference time the highest scoring candidate is selected. The detection of a preposition and its attachment candidates is done in a rule-based process (section \ref{sec:ppatt_datasets}) as a preprocessing step, which is used for building the datasets used for training and evalu
ation. They experiment with a few different scoring functions, each consists of a slightly different neural network architecture. The variations differ mostly by: (1)  the input - as one variation does not take the preposition as an input, only the head candidate and the object of the preposition; (2) the number of network layers - a single layer or two layers, and (3) the parameterization - for instance, one variation uses a different set of parameters based on the distance between the head candidate and the preposition. They also experiment with two kinds of pre-trained word embeddings: one based on the original Skip-Gram model (\cite{word2vec}) and a dependency-based variation following (\cite{bansal}), reporting significant accuracy boost when using the latter. Allowing the word embeddings to update during training was also reported to increase disambiguation accuracy significantly. 

\subsection{Model}
The following section provides a more formal description of the PP-Attachment model presented by \cite{hpcd}. For the purpose of this description, we assume a vector representation of dimension $n$ for each sentence word. The exact specification of this representation is outlined later in this section).

As described previously, the model consists of a scoring function that assigns a score to each attachment candidate. The scoring function $s(c, z, h, \theta)$ takes the vector representations of a preposition  - $z$, its child - $c$, and an attachment candidate - $h$ ($\theta$ is the set of learned model parameters). The score is defined as a linear function on a vector composed from $c, z, h$:

$$s(c,z,h,\theta) = w \cdot compose\_p(c,z,h, \theta’)$$

Where $w \in \mathbb{R}^n$ is a parameter, and $\theta'$ is the set of all parameters, excluding $w$. $compose\_p$ transforms $c, z, h$ into a single vector $p \in \mathbb{R}^n$ using parameters in $\theta’$.  The score is obtained by taking the dot product of $w$ and $p$. 

Several variations of $compose\_p$ are suggested, each corresponding to a different model variant. The top performing variant - Head-Prep-Child-Dist (HPCD) - is the one used in this work, and the only one described in detail. The rest of the variants are mainly simpler versions of HPCD.

\tocless\subsubsection{The HPCD scoring function}

The HPCD version of $compose\_p$ is defined as follows:

\begin{equation}
\begin{gathered}  
   dist = min(word\_distance(head, prep), MAX\_DIST)  \\
   p_1 = tanh(W^1[z;c] + b^1)   \\
   p_2 = tanh(W^2_{dist}[h;p_1] + b^2_{dist})     
\end{gathered}\label{eq:hpcd}
\end{equation}

Where $h, z, c$ are vectors for the head, preposition, and child respectively; $p_1, p_2$ are the output vectors for the first and second layers;
$word\_distance(head, prep)$ is the number of words between the preposition and the head candidate; $ W^1, b^1$ are the paramters for the first layer and $W^2_{dist}, b^2_{dist}$ are the distance-specific parameters of the second layer.

Namely, the HPCD scoring function is composed of a two-layered feed-forward neural network. The first layer takes the vectors for the preposition and its child, and the second layer takes the output from the first layer and the vector for the head candidate. The HPCD variant takes into account the distance between the head candidate and the preposition (i.e the number of words between them), by using different parameters for each distance, up to a predefined threshold (MAX\_DIST = 5). For any distance larger than the threshold, the parameters corresponding to threshold value are used. The model is illustrated in figure \ref{fig:hpcd}.
\\
\tocless\subsubsection{Word Vector Representation} \label{section:hpcd_wordvectors}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/hpcd_vec.pdf}
    \caption{An illustration of the extended word vectors for the head, preposition and child. In addition to the pretrained word embeddings, the vectors for the child and head are extended with WordNet, VerbNet and syntactic information. In the HPCD-PSS variant, the preposition vector is also enriched with PSS information}
    \label{fig:hpcdvectors}
\end{figure}

The word vectors for the head, preposition, and child words ($h, z, c$ in eq. \ref{eq:hpcd}) are constructed using the pretrained word embeddings of the corresponding words, and extended using auxiliary syntactic information and structured knowledge resources such as WordNet and VerbNet. They experiment with two kinds of SkipGram based word embedding models. The first is the standard SkipGram model, where the context of a word is defined by the nearby words in the sentence. The second is a variation of the standard SkipGram by \cite{bansal}, where the context of a word is defined using its head, child and corresponding dependency labels according to the dependency tree of the given sentence. These word embeddings (referred to as \textit{syntactic} embeddings) are expected to be better at capturing dependency related information and therefore be relevant for PP-Attachment disambiguation. As their experiments show, the syntactic vectors indeed yield a significant boost in accuracy. The same 100-dimensional pretrained syntactic vectors are also used in this work. 

Both child and head vectors are extended with WordNet information, by introducing a binary indicator dimension for each top hypernym of the corresponding word (the set of top hypernyms are the 26 hypernyms in top three layers of the WordNet hypernym tree). 

The head vector is also extended with binary indicators corresponding to whether the head is a noun, and in the case of a verb, whether the preposition appears in the corresponding VerbNet frame. The POS tag of the word following the head is embedded into the head vector using one-hot vector encoding. The resulting outline of the full vectors for the head, preposition and child are illustrated in figure \ref{fig:hpcdvectors}
\\
\subsection{Incorporating PSS information into HPCD} \label{sec:hpcd_pss}

The HPCD variation presented in this work (HCPD-PSS) is a simple modification of the network, allowing it to take the PSS Role and Function labels of the preposition as additional input signals. This is done by extending the preposition vector $p$ with a vector representation of each label. We experiment with two types of vector representations: binary encoding, and a lookup table. With binary encoding, we add a dimension for each PSS label in the SNACS schema. The dimensions corresponding to the correct label, and its ancestors in the SNACS label hierarchy, receive a value of 1. The binary encoding method requires fewer parameters than the lookup table method, which on the other hand potentially allows a deeper processing of the PSS signal. The dimension of lookup table vectors is considered a hyperparameter and tuned during training.

\section{Datasets} \label{sec:ppatt_datasets}

Two datasets are used for training and evaluation: one based on the STREUSLE corpus - a collection of web reviews from the English Web Treebank with human-annotated PSS labels (section \ref{sec:streusle}), and another based on the World-Street-Journal portion of the Penn-Treebank\footnote{The Penn-Treebank based PP-Attachment dataset was introduced by \cite{hpcd}, and was used to train and evaluate their models.}, enriched with automatically extracted PSS annotations using the PSS model presented in chapter \ref{Chapter2}. By evaluating the PP-Attachment models on both datasets, we obtain a rough estimation on the impact of PSS prediction errors on the task.

Both corpora are equipped with manually annotated dependency trees\footnote{The Penn-Treebank consists of constituency-based parse trees, that are converted into a dependency tree using the Penn-Dependency converter (\cite{johansson07extended}).}, from which PP-Attachment cases are extracted. The extraction process (by \cite{hpcd}) is performed as follows: For each preposition (identified by a POS tag of IN or TO) with a child (according to the dependency tree, allowing a maximum distance of 200 words between the preposition and the child), up to 10 words preceding the preposition are scanned for valid head candidates - verbs and nouns (detected according to their POS tags). The head corresponding to the correct attachment is the head of the preposition according to the dependency tree. Unambiguous prepositions are filtered out. For STREUSLE, we also filter out prepositions without a PSS annotation (about 9\% of all identified cases).

The Penn-Treebank-based dataset consists of sections 2-21 (train), 22 (dev) and 23 (test) of the PTB for English. By applying the above procedure, the 20,714/872/1157 sentences in the corpus yield 35,478/1494/1957 prepositions for the train/dev/test split. For the STREUSLE corpus, it yields 1,153/99/130 PP-Attachment cases for the 2,723/554/535 sentences in the train/dev/test split. The cases-to-sentence ratio is much larger for PTB, which generally contains longer and more complex sentences. 
    
\section{Hyperparameters and Training}

The hyperparameters used are mostly consistent with those reported by \cite{hpcd} for the original model: a Dropout probability of $p = 0.5$, $tanh$ activations and a dimension of 100 for the intermediate network layers (corresponding to $p1, p2$ from eq. \ref{eq:hpcd}). Model parameters are initialized randomly using the Xavier initialization (\cite{glorot10xavier}). Optimization is performed with respect to the Hinge Loss, using Simple Gradient Descent with Backpropagation for 100 epochs with early stopping, and batch size of 500. The validation set is used to tune the learning rate and learning rate decay parameters. It is also used to tune the PSS representation method - binary encoding or lookup table, and the representation dimensions in case of the latter. All models are implemented using the DyNet framework (\cite{dynet}). 

\section{Experiment Setup} \label{sec:ppatt_setup}

The HPCD model and its PSS-bearing variant (HPCD-PSS) are evaluated separably on the STRESULE and the Penn-Treebank (PTB) based PP-Attachment datasets. For PTB, we take the straightforward approach of using the train and development sets for training and hyperparameter tuning, and the test set for evaluation. Ideally, the same procedure would have been applied on the STREUSLE dataset, however the size of the training set (1,153 samples) is too small for the model to be trained successfully. Instead, we use the train and development sets from both datasets (assigning them with equal weights during training), and evaluate solely on the STREUSLE test set. 

\section{Results} \label{sec:ppatt_results}

\begin{table}[]
    \setlength{\tabcolsep}{10pt} % Default value: 6pt
    \renewcommand{\arraystretch}{1.5}
    \newcommand{\score}[2]{#1 {\footnotesize ($\pm$#2)}}
    \centering
    \begin{tabular}{c|c|c|c}
         Training                        & System   & PTB test acc. & STREUSLE test acc.\\ \hline
         \multirow{2}{*}{PTB}            & HPCD     & \score{87.80}{0.46} & \score{81.54}{1.88}         \\ 
                                         & HPCD-PSS & \score{88.24}{0.31} & \score{84.46}{1.48}         \\ \hline
         \multirow{2}{*}{PTB + Streusle} & HPCD     & \score{85.80}{1.88} & \score{84.62}{2.37}         \\ 
                                         & HPCD-PSS & \score{86.14}{1.42} & \score{85.85}{0.88}         \\ \hline
    \end{tabular}
    \caption{PP-Attachement disambiguation accuracy of the best performing model in each setting (average and standard deviation over 5 different training instances).}
    \label{tab:hpcdresults}
\end{table}

PP-Attachment accuracy scores for the best performing model in each setting are reported in table \ref{tab:hpcdresults}. For each system, the average accuracy and standard deviation over 5 different training instances are reported. The results somehow indicate a slight improvement in accuracy for the PSS-bearing HPCD model over vanilla HPCD (a rough 0.5 point for PTB, and 1-3 points for STREUSLE). The statistical significance of these results is unclear, as the standard deviation is well within the improvement range (0.5-1.5 points for PTB and 1-2 points for STREUSLE). Further research is thus needed to determine the significance of these accuracy gains. 

Some side observations can me made regarding the impact of training with multiple domains. As one might expect, training with both PTB and STREUSLE data decreases PTB accuracy ($\sim$2 points), and increases STREUSLE accuracy (1-3 points) when compared to training solely on PTB. The largest gain in accuracy ($\sim$3 points) is observed in the cross-domain setting, where the models are trained solely on PTB and evaluated on STREUSLE. A similar gain in performance is observed in the case of vanilla HPCD when adding STREUSLE data to the training set, which may indicate that PSS labels contribute as a normalizing factor between different domains. It is possible that this effect is observed due to an information leakage from train to test, as the PSS predictor on PTB was trained on STREUSLE. However as the only information channel is in the form of categorical PSS predictions, this seems highly unlikely.

