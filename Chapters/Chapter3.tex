% Chapter Template

\chapter{Preposition Supersenses for  Prepositional Phrase Attachment} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


%-----------------------------------
%	SECTION 1
%-----------------------------------
\section{Overview}

\section{PPA work by Belinkov et. al 2014}
\subsubsection{*Overview, relevance to this work}
This section introduces the PP-Attachment work by (Belinkov et al. 2014), which we use in our research. We incorporate PSS features onto their top-performing model, measuring the impact on accuracy. 

\subsubsection{*Intro to Belinkov work}
(Belinkov et al. 2014 todo) is perhaps the first to present a set of neural PP-Attachment models that make use of pretrained word embeddings. Their top performing model achieved state-of-the-art results, surpassing any previous attempts at this task. They experiment with different sets of word embeddings (skip-gram based and dep-gram based todo: ref), using a PP-Attachment dataset they constructed using the Penn Treebank for English and the CATiB dependency treebank (Habash and Roth, 2009) for Arabic.    

\subsubsection{* -- Isolated PPA models}
\subsubsection{* -- Incorporated to an existing parser}
The models presented operate in an isolated manner - solving PP-Attachment alone. Instead of the typical V-NP-P-NP2 formulation, which as explained in section todo was shown to be problematic, they took the more realistic approach of allowing multiple head candidates. They do, however, compare against full-scale parsers, and integrate their model into an existing one - the RGB parser (todo: ref) - resulting with an improvement of parsing accuracy in both English and Arabic when compared to using the unmodified RGB parser.

\subsubsection{* -- Model architecture in brief}
Their model architecture consists of a parameterized neural scoring function, which assigns a score to a given attachment candidate, given the corresponding word embeddings of the preposition, its child (the object) the head candidate, and their corresponding part-of-speech and WordNet, FrameNet attributes. The parameters of the scoring function are learned during training, and at inference the highest scoring candidate is chosen. They experiment with a few different scoring functions (presented below), each consists of a slightly different neural network architecture. The variations differ mostly by: (1)  the input - as one variation does not take the preposition as an input, only the head candidate and the object of the preposition; (2) the number of network layers - a single layer or two layers, and (3) the parameterization - for instance, one variation uses a different set of parameters based on the distance between the head candidate and the preposition. They also experiment with two kinds of pre-trained word embeddings: one based on the Skip-Gram model (todo: ref) and a dependency based model following (todo: ref), reporting significant accuracy boost when using the latter. Allowing the word embeddings to update during training was also reported to increase disambiguation accuracy significantly. 

\subsection{Datasets}
\subsubsection{*Structure, size and other notable stats}
\subsubsection{*Extraction process}

\subsection{Model}
\subsubsection{*Model framework}
The following section assumes that sentence words are represented using a column vector of dimension n. Section todo describes how such representation is constructed. 

As described in section todo, the model consists of a scoring function that assign a score to each attachment candidate. The scoring function is used for inference by choosing the highest scoring candidate. The detection of a preposition and its attachment candidate is not part of the model, and is done in a rule-based process as a preprocessing step, which is used for building the datasets used for training and evaluation (see section todo).

The scoring function s(c, z, h, teta) takes a preposition z with child c, and an attachment candidate h (teta is the set of model parameters to optimize during training). The score is defined to be a linear function on a vector composed from c,z,h:

s(c,z,h,teta) = w * compose\_p(c,z,h, teta’)

Where w e Rn is a parameter, and teta' is the set of all parameters except w. compose\_p transforms c, z, h into a single column vector p e Rn using parameters in teta’.  The score is obtained by taking the dot product of w and p. 

\subsubsection{*Score function variations}
Several variations of compose\_p are suggested, each corresponding to a different model variant. The top performing variant - Head-Prep-Child-Dist (HPCD) - is the one we use in our research, and the only one we describe in detail. The rest of the variants are mainly simpler versions of HPCD.

\subsubsection{The HPCD Variation}
The HPCD version of compose\_p is defined as follows:
   dist = min(word\_distance(head, prep), MAX\_DIST)
   p1 = tanh(W\_dist[z;c] + b\_dist)   
   p = tanh(W\_dist[h;p1] + b\_dist)

Where h , z , c ∈ R n are  vectors  for  the  head,  the  preposition,  and its  child respectively; p1, p ∈ R n the output vectors for the first and second layers respectively;
word\_distance(head, prep) is the number of words between the preposition and the head candidate; W\_dist ∈ Rn × 2n, b\_dist ∈ R (dist ∈ {0, .. , MAX\_DIST} ). 

Namely, the HPCD scoring function is composed of a two-layered feed-forward neural network. The first layer takes the vectors for the preposition and its child; the second layer takes the output vector from the first layer and the vector for the head candidate. The result is the output of the second layer. The HPCD variant takes into account the distance between the head candidate and the preposition (i.e the number of words between them), by using different neural network parameters for each distance, up to a predefined threshold (MAX\_DIST). For any distance larger than the threshold, the parameters corresponding to threshold value are used.

\subsection{Word Vector Representation}
\subsubsection{*Word Embedding Variations}
\subsubsection{*Vector Enrichment (WordNet, VerbNet, etc)}
\subsubsection{*Full Vector Outline}
\subsection{Training}
\subsection{Results}
\subsection{Conclusion}
\subsubsection{*Brief summary and chapter 3 outline}
\pagebreak

%-----------------------------------
%	SECTION 2
%-----------------------------------
\section{Datasets}
\subsection{World Stree Journal based dataset}
\subsubsection{*PSS enrichment of WSJ}
\subsection{STREUSLE based dataset}
\subsubsection{*Extracting PPA cases from STREUSLE}

\section{Models}
\subsection{Baseline Model: HPCD}
\subsection{HPCD extended with PSS features}

\section{Training}
\subsubsection{*Loss function and trainer remain unchanged}
\subsubsection{*Process of training on both datasets}
\subsubsection{*Hyperparameter tuning}

\section{Results}
\subsubsection{*Compare HPCD and extended HPCD on each setting and dataset}

\section{Analysis}
\subsection{Error Analysis}
\subsubsection{*Errors shared between both models}
\subsubsection{*Errors due to wrong PSS? (sampling)}

\section{Conclusions}
